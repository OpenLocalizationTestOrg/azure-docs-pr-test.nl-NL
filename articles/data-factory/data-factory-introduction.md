---
title: Inleiding tot Data Factory, een gegevensintegratieservice | Microsoft Docs
description: 'Leer wat een Data Factory is: een cloudgebaseerde gegevensintegratieservice waarmee de verplaatsing en transformatie van gegevens wordt beheerd en geautomatiseerd.'
keywords: gegevensintegratie, cloudgegevensintegratie, wat is Azure Data Factory
services: data-factory
documentationcenter: 
author: sharonlo101
manager: jhubbard
editor: monicar
ms.assetid: cec68cb5-ca0d-473b-8ae8-35de949a009e
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: get-started-article
ms.date: 08/14/2017
ms.author: shlo
ms.openlocfilehash: bc72c4d58b98f6521dbb7420a5d05a121b0ddbda
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/18/2017
---
# <a name="introduction-to-azure-data-factory"></a><span data-ttu-id="65fa6-104">Inleiding tot Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="65fa6-104">Introduction to Azure Data Factory</span></span> 
## <a name="what-is-azure-data-factory"></a><span data-ttu-id="65fa6-105">Wat is Azure Data Factory?</span><span class="sxs-lookup"><span data-stu-id="65fa6-105">What is Azure Data Factory?</span></span>
<span data-ttu-id="65fa6-106">Hoe worden zakelijke gegevens in de wereld van big data hergebruikt?</span><span class="sxs-lookup"><span data-stu-id="65fa6-106">In the world of big data, how is existing data leveraged in business?</span></span> <span data-ttu-id="65fa6-107">Is het mogelijk om gegevens die in de cloud zijn gegenereerd, waardevoller te maken met behulp van referentiegegevens uit on-premises gegevensbronnen of andere ongelijksoortige gegevensbronnen?</span><span class="sxs-lookup"><span data-stu-id="65fa6-107">Is it possible to enrich data generated in the cloud by using reference data from on-premises data sources or other disparate data sources?</span></span> <span data-ttu-id="65fa6-108">We nemen een gamingbedrijf als voorbeeld. Het verzamelt veel logboeken die wordt geproduceerd door games in de cloud.</span><span class="sxs-lookup"><span data-stu-id="65fa6-108">For example, a gaming company collects many logs produced by games in the cloud.</span></span> <span data-ttu-id="65fa6-109">Het bedrijf wil deze logboeken analyseren en inzicht krijgen in klantvoorkeuren, demografische gegevens, gebruiksgedrag, enz. Zo wil het mogelijkheden voor bijverkopen en kruisverkopen ontdekken, nieuwe interessante functies ontwikkelen om de groei van het bedrijf te stimuleren en een betere ervaring bieden aan klanten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-109">It wants to analyze these logs to gain insights in to customer preferences, demographics, usage behavior etc. to identify up-sell and cross-sell opportunities, develop new compelling features to drive business growth, and provide a better experience to customers.</span></span> 

<span data-ttu-id="65fa6-110">Voor het analyseren van deze logboeken moet het bedrijf gebruikmaken van de referentiegegevens, zoals klantgegevens, game-informatie en marketingcampagnegegevens, die zich in een on-premises gegevensarchief bevinden.</span><span class="sxs-lookup"><span data-stu-id="65fa6-110">To analyze these logs, the company needs to use the reference data such as customer information, game information, marketing campaign information that is in an on-premises data store.</span></span> <span data-ttu-id="65fa6-111">Daarom wil het bedrijf logboekgegevens opnemen uit het gegevensarchief in de cloud en gegevens uit het on-premises gegevensarchief raadplegen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-111">Therefore, the company wants to ingest log data from the cloud data store and reference data from the on-premises data store.</span></span> <span data-ttu-id="65fa6-112">Vervolgens moeten de gegevens worden verwerkt met behulp van Hadoop in de cloud (Azure HDInsight) en moeten de resulterende gegevens worden gepubliceerd in een datawarehouse in de cloud, zoals Azure SQL Data Warehouse, of een on-premises gegevensarchief zoals SQL Server.</span><span class="sxs-lookup"><span data-stu-id="65fa6-112">Then, process the data by using Hadoop in the cloud (Azure HDInsight) and publish the result data into a cloud data warehouse such as Azure SQL Data Warehouse or an on-premises data store such as SQL Server.</span></span> <span data-ttu-id="65fa6-113">Deze werkstroom moet wekelijks worden uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="65fa6-113">It wants this workflow to run weekly once.</span></span> 

<span data-ttu-id="65fa6-114">Hier is een platform nodig waarmee het bedrijf een werkstroom kan maken die gegevens uit zowel een on-premises gegevensarchief als een gegevensarchief in de cloud kan opnemen. Daarnaast moeten deze gegevens kunnen worden getransformeerd of verwerkt met behulp van bestaande rekenservices zoals Hadoop, en moeten deze worden gepubliceerd naar een on-premises gegevensarchief of een gegevensarchief in de cloud. Zo kunnen BI-toepassingen deze gegevens gebruiken.</span><span class="sxs-lookup"><span data-stu-id="65fa6-114">What is needed is a platform that allows the company to create a workflow that can ingest data from both on-premises and cloud data stores, and transform or process data by using existing compute services such as Hadoop, and publish the results to an on-premises or cloud data store for BI applications to consume.</span></span> 

![Overzicht van Data Factory](media/data-factory-introduction/what-is-azure-data-factory.png) 

<span data-ttu-id="65fa6-116">Azure Data Factory is het ideale platform voor dit soort scenario's.</span><span class="sxs-lookup"><span data-stu-id="65fa6-116">Azure Data Factory is the platform for this kind of scenarios.</span></span> <span data-ttu-id="65fa6-117">Het is een **cloud-gebaseerde gegevensintegratieservice waarmee u gegevensgestuurde werkstromen kunt maken in de cloud en zo gegevensverplaatsing en -transformatie kunt indelen en automatiseren**.</span><span class="sxs-lookup"><span data-stu-id="65fa6-117">It is a **cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation**.</span></span> <span data-ttu-id="65fa6-118">Met Azure Data Factory kunt u gegevensgestuurde werkstromen (ook wel pijplijnen) maken en plannen die gegevens uit verschillende gegevensarchieven kunnen opnemen en de gegevens kunnen verwerken/transformeren met behulp van rekenservices zoals Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics en Azure Machine Learning. Daarnaast kunt u de uitvoergegevens publiceren naar gegevensarchieven zoals Azure SQL Data Warehouse, zodat BI-toepassingen (business intelligence) ze kunnen gebruiken.</span><span class="sxs-lookup"><span data-stu-id="65fa6-118">Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores, process/transform the data by using compute services such as Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, and Azure Machine Learning, and publish output data to data stores such as Azure SQL Data Warehouse for business intelligence (BI) applications to consume.</span></span>  

<span data-ttu-id="65fa6-119">Het is eerder een EL-platform (Extract-and-Load) en vervolgens een TL-platform (Transform-and-Load) dan een traditioneel ETL-platform (Extract-Transform-and-Load).</span><span class="sxs-lookup"><span data-stu-id="65fa6-119">It's more of an Extract-and-Load (EL) and then Transform-and-Load (TL) platform rather than a traditional Extract-Transform-and-Load (ETL) platform.</span></span> <span data-ttu-id="65fa6-120">De transformaties die worden uitgevoerd zijn transformaties/verwerkingen van gegevens met behulp van rekenservices in plaats van uitvoertransformaties, zoals die voor het toevoegen van afgeleide kolommen, het tellen van het aantal rijen, het sorteren van gegevens, enzovoort.</span><span class="sxs-lookup"><span data-stu-id="65fa6-120">The transformations that are performed are to transform/process data by using compute services rather than to perform transformations like the ones for adding derived columns, counting number of rows, sorting data, etc.</span></span> 

<span data-ttu-id="65fa6-121">De gegevens die in Azure Data Factory worden verwerkt en geproduceerd door werkstromen zijn momenteel **tijdgesegmenteerd** (per uur, dagelijks, wekelijks, enzovoort).</span><span class="sxs-lookup"><span data-stu-id="65fa6-121">Currently, in Azure Data Factory, the data that is consumed and produced by workflows is **time-sliced data** (hourly, daily, weekly, etc.).</span></span> <span data-ttu-id="65fa6-122">Een pijplijn kan bijvoorbeeld één keer per dag invoergegevens lezen, gegevens verwerken en uitvoer produceren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-122">For example, a pipeline may read input data, process data, and produce output data once a day.</span></span> <span data-ttu-id="65fa6-123">U kunt er ook voor kiezen om een werkstroom slechts één keer uit te voeren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-123">You can also run a workflow just one time.</span></span>  
  

## <a name="how-does-it-work"></a><span data-ttu-id="65fa6-124">Hoe werkt het?</span><span class="sxs-lookup"><span data-stu-id="65fa6-124">How does it work?</span></span> 
<span data-ttu-id="65fa6-125">De pijplijnen (gegevensgestuurde werkstromen) in Azure Data Factory voeren normaal gesproken de volgende drie stappen uit:</span><span class="sxs-lookup"><span data-stu-id="65fa6-125">The pipelines (data-driven workflows) in Azure Data Factory typically perform the following three steps:</span></span>

![Drie fasen van Azure Data Factory](media/data-factory-introduction/three-information-production-stages.png)

### <a name="connect-and-collect"></a><span data-ttu-id="65fa6-127">Verbinding maken en verzamelen</span><span class="sxs-lookup"><span data-stu-id="65fa6-127">Connect and collect</span></span>
<span data-ttu-id="65fa6-128">Ondernemingen hebben verschillende typen gegevens, opgeslagen op verschillende bronnen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-128">Enterprises have data of various types located in disparate sources.</span></span> <span data-ttu-id="65fa6-129">De eerste stap voor het bouwen van een informatieproductiesysteem bestaat uit het verbinden van alle vereiste gegevens- en verwerkingsbronnen, zoals SaaS-services, bestandsshares, FTP en webservices, en het zo nodig verplaatsen van gegevens naar een centrale locatie voor verdere verwerking.</span><span class="sxs-lookup"><span data-stu-id="65fa6-129">The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed to a centralized location for subsequent processing.</span></span>

<span data-ttu-id="65fa6-130">Zonder Data Factory moeten ondernemingen aangepaste onderdelen voor gegevensverplaatsing ontwikkelen of aangepaste services schrijven om deze gegevensbronnen en verwerking te integreren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-130">Without Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.</span></span> <span data-ttu-id="65fa6-131">Het is duur en lastig om dergelijke systemen te integreren en te onderhouden, en vaak ontbreken de adequate bewakingssystemen, waarschuwingssystemen en besturingselementen die een volledig beheerde service wel kan bieden.</span><span class="sxs-lookup"><span data-stu-id="65fa6-131">It is expensive and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.</span></span>

<span data-ttu-id="65fa6-132">Met Data Factory kunt u de kopieeractiviteit in een pijplijn gebruiken om gegevens van on-premises gegevensarchieven en gegevensarchieven uit de cloud te verplaatsen naar een gecentraliseerd gegevensarchief in de cloud voor verdere analyse.</span><span class="sxs-lookup"><span data-stu-id="65fa6-132">With Data Factory, you can use the Copy Activity in a data pipeline to move data from both on-premises and cloud source data stores to a centralization data store in the cloud for further analysis.</span></span> <span data-ttu-id="65fa6-133">Zo kunt u bijvoorbeeld gegevens in Azure Data Lake Store verzamelen en later transformeren met behulp van een Azure Data Lake Analytics Compute-service.</span><span class="sxs-lookup"><span data-stu-id="65fa6-133">For example, you can collect data in an Azure Data Lake Store and transform the data later by using an Azure Data Lake Analytics compute service.</span></span> <span data-ttu-id="65fa6-134">Of u kunt gegevens verzamelen in Azure Blob Storage en later transformeren met behulp van een Hadoop-cluster van Azure HDInsight.</span><span class="sxs-lookup"><span data-stu-id="65fa6-134">Or, collect data in an Azure Blob Storage and transform data later by using an Azure HDInsight Hadoop cluster.</span></span>

### <a name="transform-and-enrich"></a><span data-ttu-id="65fa6-135">Transformeren en verrijken</span><span class="sxs-lookup"><span data-stu-id="65fa6-135">Transform and enrich</span></span>
<span data-ttu-id="65fa6-136">Als gegevens aanwezig zijn in een gecentraliseerd gegevensarchief in de cloud, wilt u dat de verzamelde gegevens worden verwerkt of getransformeerd met behulp van rekenservices zoals HDInsight Hadoop, Spark, Data Lake Analytics of Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="65fa6-136">Once data is present in a centralized data store in the cloud, you want the collected data to be processed or transformed by using compute services such as HDInsight Hadoop, Spark, Data Lake Analytics, and Machine Learning.</span></span> <span data-ttu-id="65fa6-137">U wilt dat de gegevens op een betrouwbare manier en volgens een beheersbare en gecontroleerde planning worden geproduceerd om productieomgevingen te voorzien van vertrouwde gegevens.</span><span class="sxs-lookup"><span data-stu-id="65fa6-137">You want to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.</span></span> 

### <a name="publish"></a><span data-ttu-id="65fa6-138">Publiceren</span><span class="sxs-lookup"><span data-stu-id="65fa6-138">Publish</span></span> 
<span data-ttu-id="65fa6-139">Lever vanuit de cloud getransformeerde gegevens aan on-premises bronnen zoals SQL Server, of sla de informatie op in uw cloudopslagbronnen voor gebruik door BI- (Business Intelligence) en analysehulpprogramma's en andere toepassingen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-139">Deliver transformed data from the cloud to on-premises sources like SQL Server, or keep it in your cloud storage sources for consumption by business intelligence (BI) and analytics tools and other applications.</span></span>

## <a name="key-components"></a><span data-ttu-id="65fa6-140">Belangrijkste onderdelen</span><span class="sxs-lookup"><span data-stu-id="65fa6-140">Key components</span></span>
<span data-ttu-id="65fa6-141">Een Azure-abonnement kan een of meer Azure Data Factory-exemplaren (oftewel 'data factory's') hebben.</span><span class="sxs-lookup"><span data-stu-id="65fa6-141">An Azure subscription may have one or more Azure Data Factory instances (or data factories).</span></span> <span data-ttu-id="65fa6-142">Azure Data Factory bestaat uit vier hoofdonderdelen die samenwerken om een platform te bieden waarop u voor uw gegevensgestuurde werkstromen kunt maken met stappen voor de verplaatsing en transformatie van gegevens.</span><span class="sxs-lookup"><span data-stu-id="65fa6-142">Azure Data Factory is composed of four key components that work together to provide the platform on which you can compose data-driven workflows with steps to move and transform data.</span></span> 

### <a name="pipeline"></a><span data-ttu-id="65fa6-143">Pijplijn</span><span class="sxs-lookup"><span data-stu-id="65fa6-143">Pipeline</span></span>
<span data-ttu-id="65fa6-144">Een data factory kan één of meer pijplijnen hebben.</span><span class="sxs-lookup"><span data-stu-id="65fa6-144">A data factory may have one or more pipelines.</span></span> <span data-ttu-id="65fa6-145">Een pijplijn is een groep activiteiten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-145">A pipeline is a group of activities.</span></span> <span data-ttu-id="65fa6-146">De activiteiten in een pijplijn voeren samen een taak uit.</span><span class="sxs-lookup"><span data-stu-id="65fa6-146">Together, the activities in a pipeline perform a task.</span></span> <span data-ttu-id="65fa6-147">Zo kan een pijplijn bijvoorbeeld een groep activiteiten bevatten die gegevens van een Azure-blob opnemen en vervolgens een Hive-query uitvoeren op een HDInsight-cluster om de gegevens te partitioneren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-147">For example, a pipeline could contain a group of activities that ingests data from an Azure blob, and then run a Hive query on an HDInsight cluster to partition the data.</span></span> <span data-ttu-id="65fa6-148">Het voordeel van een pijplijn is dat u de activiteiten kunt beheren als een groep in plaats van afzonderlijke activiteiten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-148">The benefit of this is that the pipeline allows you to manage the activities as a set instead of each one individually.</span></span> <span data-ttu-id="65fa6-149">U kunt de pijplijn bijvoorbeeld in zijn geheel implementeren en plannen, in plaats van de afzonderlijke activiteiten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-149">For example, you can deploy and schedule the pipeline, instead of the activities independently.</span></span> 

### <a name="activity"></a><span data-ttu-id="65fa6-150">Activiteit</span><span class="sxs-lookup"><span data-stu-id="65fa6-150">Activity</span></span>
<span data-ttu-id="65fa6-151">Een pijplijn kan één of meer activiteiten bevatten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-151">A pipeline may have one or more activities.</span></span> <span data-ttu-id="65fa6-152">Met activiteiten definieert u welk acties moeten worden uitgevoerd voor uw gegevens.</span><span class="sxs-lookup"><span data-stu-id="65fa6-152">Activities define the actions to perform on your data.</span></span> <span data-ttu-id="65fa6-153">U kunt bijvoorbeeld een kopieeractiviteit gebruiken om gegevens van één gegevensarchief naar een andere te kopiëren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-153">For example, you may use a Copy activity to copy data from one data store to another data store.</span></span> <span data-ttu-id="65fa6-154">U kunt ook een Hive-activiteit gebruiken, waarmee een Hive-query wordt uitgevoerd voor een Azure HDInsight-cluster om uw gegevens te transformeren of analyseren.</span><span class="sxs-lookup"><span data-stu-id="65fa6-154">Similarly, you may use a Hive activity, which runs a Hive query on an Azure HDInsight cluster to transform or analyze your data.</span></span> <span data-ttu-id="65fa6-155">Data Factory ondersteunt twee soorten activiteiten: activiteiten voor gegevensverplaatsing en activiteiten voor gegevenstransformatie.</span><span class="sxs-lookup"><span data-stu-id="65fa6-155">Data Factory supports two types of activities: data movement activities and data transformation activities.</span></span>

### <a name="data-movement-activities"></a><span data-ttu-id="65fa6-156">Activiteiten voor gegevensverplaatsing</span><span class="sxs-lookup"><span data-stu-id="65fa6-156">Data movement activities</span></span>
<span data-ttu-id="65fa6-157">De kopieeractiviteit in Data Factory kopieert gegevens van een brongegevensarchief naar een sinkgegevensarchief.</span><span class="sxs-lookup"><span data-stu-id="65fa6-157">Copy Activity in Data Factory copies data from a source data store to a sink data store.</span></span> <span data-ttu-id="65fa6-158">Data Factory ondersteunt de volgende gegevensarchieven.</span><span class="sxs-lookup"><span data-stu-id="65fa6-158">Data Factory supports the following data stores.</span></span> <span data-ttu-id="65fa6-159">Gegevens vanuit elke willekeurige bron kunnen naar een sink worden geschreven.</span><span class="sxs-lookup"><span data-stu-id="65fa6-159">Data from any source can be written to any sink.</span></span> <span data-ttu-id="65fa6-160">Klik op een gegevensarchief voor informatie over het kopiëren van gegevens naar en van dat archief.</span><span class="sxs-lookup"><span data-stu-id="65fa6-160">Click a data store to learn how to copy data to and from that store.</span></span>

[!INCLUDE [data-factory-supported-data-stores](../../includes/data-factory-supported-data-stores.md)]

<span data-ttu-id="65fa6-161">Zie het artikel [Activiteiten voor gegevensverplaatsing](data-factory-data-movement-activities.md) voor meer informatie.</span><span class="sxs-lookup"><span data-stu-id="65fa6-161">For more information, see [Data Movement Activities](data-factory-data-movement-activities.md) article.</span></span>

### <a name="data-transformation-activities"></a><span data-ttu-id="65fa6-162">Activiteiten voor gegevenstransformatie</span><span class="sxs-lookup"><span data-stu-id="65fa6-162">Data transformation activities</span></span>
[!INCLUDE [data-factory-transformation-activities](../../includes/data-factory-transformation-activities.md)]

<span data-ttu-id="65fa6-163">Zie het artikel [Activiteiten voor gegevenstransformatie](data-factory-data-transformation-activities.md) voor meer informatie.</span><span class="sxs-lookup"><span data-stu-id="65fa6-163">For more information, see [Data Transformation Activities](data-factory-data-transformation-activities.md) article.</span></span>

### <a name="custom-net-activities"></a><span data-ttu-id="65fa6-164">Aangepaste .NET-activiteiten</span><span class="sxs-lookup"><span data-stu-id="65fa6-164">Custom .NET activities</span></span>
<span data-ttu-id="65fa6-165">Als u gegevens wilt verplaatsen naar/van een gegevensarchief dat niet wordt ondersteund door de kopieeractiviteit, of gegevens wilt transformeren met behulp van uw eigen logica, maakt u een **aangepaste .NET-activiteit**.</span><span class="sxs-lookup"><span data-stu-id="65fa6-165">If you need to move data to/from a data store that Copy Activity doesn't support, or transform data using your own logic, create a **custom .NET activity**.</span></span> <span data-ttu-id="65fa6-166">Zie [Aangepaste activiteiten gebruiken in een Azure Data Factory-pijplijn](data-factory-use-custom-activities.md) voor meer informatie over het maken en gebruiken van een aangepaste activiteit.</span><span class="sxs-lookup"><span data-stu-id="65fa6-166">For details on creating and using a custom activity, see [Use custom activities in an Azure Data Factory pipeline](data-factory-use-custom-activities.md).</span></span>

### <a name="datasets"></a><span data-ttu-id="65fa6-167">Gegevenssets</span><span class="sxs-lookup"><span data-stu-id="65fa6-167">Datasets</span></span>
<span data-ttu-id="65fa6-168">Voor een activiteit zijn nul of meer gegevenssets nodig als invoer en één of meer gegevenssets als uitvoer.</span><span class="sxs-lookup"><span data-stu-id="65fa6-168">An activity takes zero or more datasets as inputs and one or more datasets as outputs.</span></span> <span data-ttu-id="65fa6-169">Gegevenssets vertegenwoordigen gegevensstructuren in de gegevensarchieven die simpelweg verwijzen naar de gegevens die u in uw activiteiten als in- of uitvoer wilt gebruiken.</span><span class="sxs-lookup"><span data-stu-id="65fa6-169">Datasets represent data structures within the data stores, which simply point or reference the data you want to use in your activities as inputs or outputs.</span></span> <span data-ttu-id="65fa6-170">Een Azure Blob-gegevensset benoemt bijvoorbeeld de blobcontainer en -map in de Azure Blob Storage van waaruit de pijplijn de gegevens moet lezen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-170">For example, an Azure Blob dataset specifies the blob container and folder in the Azure Blob Storage from which the pipeline should read the data.</span></span> <span data-ttu-id="65fa6-171">Of een Azure SQL Table-gegevensset bevat de tabel waar de uitvoergegevens worden geschreven door de activiteit.</span><span class="sxs-lookup"><span data-stu-id="65fa6-171">Or, an Azure SQL Table dataset specifies the table to which the output data is written by the activity.</span></span> 

### <a name="linked-services"></a><span data-ttu-id="65fa6-172">Gekoppelde services</span><span class="sxs-lookup"><span data-stu-id="65fa6-172">Linked services</span></span>
<span data-ttu-id="65fa6-173">Gekoppelde services zijn te vergelijken met verbindingsreeksen, die de verbindingsinformatie bevatten die Data Factory nodig heeft om verbinding te maken met externe bronnen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-173">Linked services are much like connection strings, which define the connection information needed for Data Factory to connect to external resources.</span></span> <span data-ttu-id="65fa6-174">Als u het op deze manier bekijkt, vertegenwoordigt de gegevensset in feite de structuur van de gegevens en definieert de gekoppelde service de verbinding met de gegevensbron.</span><span class="sxs-lookup"><span data-stu-id="65fa6-174">Think of it this way - a linked service defines the connection to the data source and a dataset represents the structure of the data.</span></span> <span data-ttu-id="65fa6-175">Een met Azure Storage gekoppelde service geeft bijvoorbeeld een verbindingsreeks zodat deze verbinding kan maken met het Azure Storage-account.</span><span class="sxs-lookup"><span data-stu-id="65fa6-175">For example, an Azure Storage linked service specifies connection string to connect to the Azure Storage account.</span></span> <span data-ttu-id="65fa6-176">En een Azure Blob-gegevenssets geeft de blobcontainer op, en de map met de gegevens.</span><span class="sxs-lookup"><span data-stu-id="65fa6-176">And, an Azure Blob dataset specifies the blob container and the folder that contains the data.</span></span>   

<span data-ttu-id="65fa6-177">Gekoppelde services worden voor twee doeleinden gebruikt in een Data Factory:</span><span class="sxs-lookup"><span data-stu-id="65fa6-177">Linked services are used for two purposes in Data Factory:</span></span>

* <span data-ttu-id="65fa6-178">Als vertegenwoordiging van een **gegevensarchief**, zoals een on-premises SQL Server, een Oracle-database, een bestandsshare of een Azure Blob Storage-account.</span><span class="sxs-lookup"><span data-stu-id="65fa6-178">To represent a **data store** including, but not limited to, an on-premises SQL Server, Oracle database, file share, or an Azure Blob Storage account.</span></span> <span data-ttu-id="65fa6-179">Zie het gedeelte [Activiteiten voor gegevensverplaatsing](#data-movement-activities) voor een lijst met ondersteunde gegevensarchieven.</span><span class="sxs-lookup"><span data-stu-id="65fa6-179">See the [Data movement activities](#data-movement-activities) section for a list of supported data stores.</span></span>
* <span data-ttu-id="65fa6-180">Ter vertegenwoordiging van een **rekenresource** die de uitvoering van een activiteit kan hosten.</span><span class="sxs-lookup"><span data-stu-id="65fa6-180">To represent a **compute resource** that can host the execution of an activity.</span></span> <span data-ttu-id="65fa6-181">De activiteit HDInsightHive wordt bijvoorbeeld uitgevoerd in een HDInsight Hadoop-cluster.</span><span class="sxs-lookup"><span data-stu-id="65fa6-181">For example, the HDInsightHive activity runs on an HDInsight Hadoop cluster.</span></span> <span data-ttu-id="65fa6-182">Zie de sectie [Activiteiten voor gegevenstransformatie](#data-transformation-activities) voor een lijst met ondersteunde rekenomgevingen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-182">See [Data transformation activities](#data-transformation-activities) section for a list of supported compute environments.</span></span>

### <a name="relationship-between-data-factory-entities"></a><span data-ttu-id="65fa6-183">Relatie tussen Data Factory-entiteiten</span><span class="sxs-lookup"><span data-stu-id="65fa6-183">Relationship between Data Factory entities</span></span>
<span data-ttu-id="65fa6-184">![Diagram: Data Factory, een service voor gegevensintegratie in de cloud - belangrijkste concepten](./media/data-factory-introduction/data-integration-service-key-concepts.png)
**Afbeelding 2.**</span><span class="sxs-lookup"><span data-stu-id="65fa6-184">![Diagram: Data Factory, a cloud data integration service - Key Concepts](./media/data-factory-introduction/data-integration-service-key-concepts.png)
**Figure 2.**</span></span> <span data-ttu-id="65fa6-185">Relaties tussen de gegevensset, de activiteit, de pijplijn en de gekoppelde service</span><span class="sxs-lookup"><span data-stu-id="65fa6-185">Relationships between Dataset, Activity, Pipeline, and Linked service</span></span>

## <a name="supported-regions"></a><span data-ttu-id="65fa6-186">Ondersteunde regio’s</span><span class="sxs-lookup"><span data-stu-id="65fa6-186">Supported regions</span></span>
<span data-ttu-id="65fa6-187">U kunt op dit moment gegevensfactory’s maken in de regio’s **VS - west**, **VS - oost** en **Noord-Europa**.</span><span class="sxs-lookup"><span data-stu-id="65fa6-187">Currently, you can create data factories in the **West US**, **East US**, and **North Europe** regions.</span></span> <span data-ttu-id="65fa6-188">Een gegevensfactory heeft echter wel toegang tot gegevensarchieven en Compute Services in andere Azure-regio’s om gegevens te verplaatsen tussen gegevensarchieven of om gegevens te verwerken middels Compute Services.</span><span class="sxs-lookup"><span data-stu-id="65fa6-188">However, a data factory can access data stores and compute services in other Azure regions to move data between data stores or process data using compute services.</span></span>

<span data-ttu-id="65fa6-189">Azure Data Factory zelf slaat geen gegevens op.</span><span class="sxs-lookup"><span data-stu-id="65fa6-189">Azure Data Factory itself does not store any data.</span></span> <span data-ttu-id="65fa6-190">U kunt er gegevensgestuurde werkstromen mee maken om de verplaatsing van gegevens te beheren tussen [ondersteunde gegevensarchieven](#data-movement-activities) en om er gegevens mee te verwerken middels [Compute Services](#data-transformation-activities) in andere regio's of in een on-premises omgeving.</span><span class="sxs-lookup"><span data-stu-id="65fa6-190">It lets you create data-driven workflows to orchestrate movement of data between [supported data stores](#data-movement-activities) and processing of data using [compute services](#data-transformation-activities) in other regions or in an on-premises environment.</span></span> <span data-ttu-id="65fa6-191">U kunt er ook [werkstromen mee bewaken en beheren](data-factory-monitor-manage-pipelines.md) met zowel programmatische als gebruikersinterfacemechanismen.</span><span class="sxs-lookup"><span data-stu-id="65fa6-191">It also allows you to [monitor and manage workflows](data-factory-monitor-manage-pipelines.md) using both programmatic and UI mechanisms.</span></span>

<span data-ttu-id="65fa6-192">Hoewel Data Factory alleen beschikbaar is in **VS - west**, **VS - oost** en **Noord-Europa**, is de service die gegevensverplaatsing in Data Factory mogelijk maakt, [wereldwijd](data-factory-data-movement-activities.md#global) beschikbaar in meerdere regio’s.</span><span class="sxs-lookup"><span data-stu-id="65fa6-192">Even though Data Factory is available in only **West US**, **East US**, and **North Europe** regions, the service powering the data movement in Data Factory is available [globally](data-factory-data-movement-activities.md#global) in several regions.</span></span> <span data-ttu-id="65fa6-193">Als een gegevensarchief zich achter een firewall bevindt, worden de gegevens verplaatst middels een [gegevensbeheergateway](data-factory-move-data-between-onprem-and-cloud.md) die is geïnstalleerd in uw on-premises omgeving.</span><span class="sxs-lookup"><span data-stu-id="65fa6-193">If a data store is behind a firewall, then a [Data Management Gateway](data-factory-move-data-between-onprem-and-cloud.md) installed in your on-premises environment moves the data instead.</span></span>

<span data-ttu-id="65fa6-194">Voorbeeld: uw berekeningsomgevingen, zoals een Azure HDInsight-cluster en Azure Machine Learning, worden uitgevoerd in de regio West-Europa.</span><span class="sxs-lookup"><span data-stu-id="65fa6-194">For an example, let us assume that your compute environments such as Azure HDInsight cluster and Azure Machine Learning are running out of West Europe region.</span></span> <span data-ttu-id="65fa6-195">U kunt een Azure Data Factory-exemplaar maken en gebruiken in Noord-Europa en dit gebruiken om taken te plannen in uw berekeningsomgevingen in West-Europa.</span><span class="sxs-lookup"><span data-stu-id="65fa6-195">You can create and use an Azure Data Factory instance in North Europe and use it to schedule jobs on your compute environments in West Europe.</span></span> <span data-ttu-id="65fa6-196">Het duurt enkele milliseconden voordat Data Factory de taak in uw berekeningsomgeving activeert, maar de uitvoertijd van de taak verandert niet in uw berekeningsomgeving.</span><span class="sxs-lookup"><span data-stu-id="65fa6-196">It takes a few milliseconds for Data Factory to trigger the job on your compute environment but the time for running the job on your computing environment does not change.</span></span>

## <a name="get-started-with-creating-a-pipeline"></a><span data-ttu-id="65fa6-197">Aan de slag met het maken van een pijplijn</span><span class="sxs-lookup"><span data-stu-id="65fa6-197">Get started with creating a pipeline</span></span>
<span data-ttu-id="65fa6-198">U kunt een van deze hulpprogramma's of API's gebruiken om gegevenspijplijnen te maken in Azure Data Factory:</span><span class="sxs-lookup"><span data-stu-id="65fa6-198">You can use one of these tools or APIs to create data pipelines in Azure Data Factory:</span></span> 

- <span data-ttu-id="65fa6-199">Azure Portal</span><span class="sxs-lookup"><span data-stu-id="65fa6-199">Azure portal</span></span>
- <span data-ttu-id="65fa6-200">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="65fa6-200">Visual Studio</span></span>
- <span data-ttu-id="65fa6-201">PowerShell</span><span class="sxs-lookup"><span data-stu-id="65fa6-201">PowerShell</span></span>
- <span data-ttu-id="65fa6-202">.NET API</span><span class="sxs-lookup"><span data-stu-id="65fa6-202">.NET API</span></span>
- <span data-ttu-id="65fa6-203">REST API</span><span class="sxs-lookup"><span data-stu-id="65fa6-203">REST API</span></span>
- <span data-ttu-id="65fa6-204">Azure Resource Manager-sjabloon.</span><span class="sxs-lookup"><span data-stu-id="65fa6-204">Azure Resource Manager template.</span></span> 

<span data-ttu-id="65fa6-205">Volg de stapsgewijze instructies in de onderstaande zelfstudies voor informatie over het bouwen van gegevensfactory’s met gegevenspijplijnen:</span><span class="sxs-lookup"><span data-stu-id="65fa6-205">To learn how to build data factories with data pipelines, follow step-by-step instructions in the following tutorials:</span></span>

| <span data-ttu-id="65fa6-206">Zelfstudie</span><span class="sxs-lookup"><span data-stu-id="65fa6-206">Tutorial</span></span> | <span data-ttu-id="65fa6-207">Beschrijving</span><span class="sxs-lookup"><span data-stu-id="65fa6-207">Description</span></span> |
| --- | --- |
| [<span data-ttu-id="65fa6-208">Gegevens verplaatsen tussen twee cloudlocaties voor gegevensopslag</span><span class="sxs-lookup"><span data-stu-id="65fa6-208">Move data between two cloud data stores</span></span>](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |<span data-ttu-id="65fa6-209">In deze zelfstudie maakt u een gegevensfactory met een pijplijn die **gegevens verplaatst** van Blob Storage naar SQL Database.</span><span class="sxs-lookup"><span data-stu-id="65fa6-209">In this tutorial, you create a data factory with a pipeline that **moves data** from Blob storage to SQL database.</span></span> |
| [<span data-ttu-id="65fa6-210">Gegevens transformeren met een Hadoop-cluster</span><span class="sxs-lookup"><span data-stu-id="65fa6-210">Transform data using Hadoop cluster</span></span>](data-factory-build-your-first-pipeline.md) |<span data-ttu-id="65fa6-211">In deze zelfstudie bouwt u uw eerste Azure-gegevensfactory met een gegevenspijplijn die **gegevens verwerkt** door Hive-script uit te voeren op een Azure HDInsight-cluster (Hadoop).</span><span class="sxs-lookup"><span data-stu-id="65fa6-211">In this tutorial, you build your first Azure data factory with a data pipeline that **processes data** by running Hive script on an Azure HDInsight (Hadoop) cluster.</span></span> |
| [<span data-ttu-id="65fa6-212">Gegevens verplaatsen tussen een on-premises gegevensopslag en een gegevensarchief in de cloud met behulp van Data Management Gateway</span><span class="sxs-lookup"><span data-stu-id="65fa6-212">Move data between an on-premises data store and a cloud data store using Data Management Gateway</span></span>](data-factory-move-data-between-onprem-and-cloud.md) |<span data-ttu-id="65fa6-213">In deze zelfstudie maakt u een gegevensfactory met een pijplijn die **gegevens verplaatst** van een **on-premises** SQL Server-database naar een Azure-blob.</span><span class="sxs-lookup"><span data-stu-id="65fa6-213">In this tutorial, you build a data factory with a pipeline that **moves data** from an **on-premises** SQL Server database to an Azure blob.</span></span> <span data-ttu-id="65fa6-214">Als onderdeel van de procedure installeert en configureert u de gegevensbeheergateway op uw computer.</span><span class="sxs-lookup"><span data-stu-id="65fa6-214">As part of the walkthrough, you install and configure the Data Management Gateway on your machine.</span></span> |

---
title: Overzicht van Gegevenswetenschap met Spark op Azure HDInsight | Microsoft Docs
description: De toolkit Spark MLlib biedt aanzienlijke machine learning modelleren mogelijkheden voor de gedistribueerde HDInsight-omgeving.
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: a4e1de99-a554-4240-9647-2c6d669593c8
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/15/2017
ms.author: deguhath;bradsev;gokuma
ms.openlocfilehash: 379b32f4e533f48f1593a97e73737a0c5bfb9135
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 07/11/2017
---
# <a name="overview-of-data-science-using-spark-on-azure-hdinsight"></a><span data-ttu-id="73a2e-103">Overzicht van gegevenswetenschap met Spark op Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="73a2e-103">Overview of data science using Spark on Azure HDInsight</span></span>
[!INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

<span data-ttu-id="73a2e-104">Deze reeks onderwerpen wordt beschreven hoe HDInsight Spark gebruiken om algemene gegevens wetenschappelijke taken zoals gegevensopname, functie-engineering, modellering en evaluatie van het model te voltooien.</span><span class="sxs-lookup"><span data-stu-id="73a2e-104">This suite of topics shows how to use HDInsight Spark to complete common data science tasks such as data ingestion, feature engineering, modeling, and model evaluation.</span></span> <span data-ttu-id="73a2e-105">De gegevens die worden gebruikt, is een voorbeeld van de 2013 NYC taxi reis en tarief gegevensset.</span><span class="sxs-lookup"><span data-stu-id="73a2e-105">The data used is a sample of the 2013 NYC taxi trip and fare dataset.</span></span> <span data-ttu-id="73a2e-106">De modellen gebouwd zijn logistic en lineaire regressie, willekeurige forests en kleurovergang gestimuleerd structuren.</span><span class="sxs-lookup"><span data-stu-id="73a2e-106">The models built include logistic and linear regression, random forests, and gradient boosted trees.</span></span> <span data-ttu-id="73a2e-107">De onderwerpen bevatten ook het opslaan van deze modellen in Azure blob storage (WASB) en hoe beoordelen en evalueren hun voorspellende prestaties.</span><span class="sxs-lookup"><span data-stu-id="73a2e-107">The topics also show how to store these models in Azure blob storage (WASB) and how to score and evaluate their predictive performance.</span></span> <span data-ttu-id="73a2e-108">Meer geavanceerde onderwerpen wordt uitgelegd hoe modellen kunnen worden getraind met sweeping kruisvalidatie en hyper-parameter.</span><span class="sxs-lookup"><span data-stu-id="73a2e-108">More advanced topics cover how models can be trained using cross-validation and hyper-parameter sweeping.</span></span> <span data-ttu-id="73a2e-109">In dit onderwerp verwijst ook naar de onderwerpen over het instellen van het Spark-cluster moet u de stappen in de scenario's.</span><span class="sxs-lookup"><span data-stu-id="73a2e-109">This overview topic also references the topics that describe how to set up the Spark cluster that you need to complete the steps in the walkthroughs provided.</span></span> 

## <a name="spark-and-mllib"></a><span data-ttu-id="73a2e-110">Spark en MLlib</span><span class="sxs-lookup"><span data-stu-id="73a2e-110">Spark and MLlib</span></span>
<span data-ttu-id="73a2e-111">[Spark](http://spark.apache.org/) is een parallelle verwerking van open-source framework die ondersteuning biedt voor in-memory verwerking naar het verbeteren de prestaties van analytische big data-toepassingen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-111">[Spark](http://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</span></span> <span data-ttu-id="73a2e-112">De Spark-verwerkingsengine is gebouwd voor snelheid, gebruiksgemak en geavanceerde analyses.</span><span class="sxs-lookup"><span data-stu-id="73a2e-112">The Spark processing engine is built for speed, ease of use, and sophisticated analytics.</span></span> <span data-ttu-id="73a2e-113">De Spark in-memory gedistribueerde rekencapaciteiten kunnen u een goede keuze voor de herhalende algoritmen in machine learning- en grafiekberekeningen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-113">Spark's in-memory distributed computation capabilities make it a good choice for the iterative algorithms used in machine learning and graph computations.</span></span> <span data-ttu-id="73a2e-114">[MLlib](http://spark.apache.org/mllib/) is modelleren van Spark schaalbare machine learning-bibliotheek die ook de algoritmische mogelijkheden aan deze gedistribueerde omgeving.</span><span class="sxs-lookup"><span data-stu-id="73a2e-114">[MLlib](http://spark.apache.org/mllib/) is Spark's scalable machine learning library that brings the algorithmic modeling capabilities to this distributed environment.</span></span> 

## <a name="hdinsight-spark"></a><span data-ttu-id="73a2e-115">Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="73a2e-115">HDInsight Spark</span></span>
<span data-ttu-id="73a2e-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is de Azure gehoste oplossing van open-source Spark.</span><span class="sxs-lookup"><span data-stu-id="73a2e-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is the Azure hosted offering of open-source Spark.</span></span> <span data-ttu-id="73a2e-117">Biedt ook ondersteuning voor **Jupyter PySpark-notebooks** op het Spark-cluster dat Spark SQL interactieve query's voor het transformeren, te filteren en visualiseren van gegevens die zijn opgeslagen in Azure BLOB's (WASB) kan worden uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="73a2e-117">It also includes support for **Jupyter PySpark notebooks** on the Spark cluster that can run Spark SQL interactive queries for transforming, filtering, and visualizing data stored in Azure Blobs (WASB).</span></span> <span data-ttu-id="73a2e-118">PySpark is de Python-API voor Spark.</span><span class="sxs-lookup"><span data-stu-id="73a2e-118">PySpark is the Python API for Spark.</span></span> <span data-ttu-id="73a2e-119">De codefragmenten die voorzien van de oplossingen en weergeven van de relevante waarnemingspunten om de gegevens die hier worden uitgevoerd in Jupyter-notebooks ge√Ønstalleerd op de Spark-clusters te visualiseren.</span><span class="sxs-lookup"><span data-stu-id="73a2e-119">The code snippets that provide the solutions and show the relevant plots to visualize the data here run in Jupyter notebooks installed on the Spark clusters.</span></span> <span data-ttu-id="73a2e-120">De stappen modelleren in de volgende onderwerpen bevatten code die laat hoe trainen zien, evalueren, opslaan en gebruiken van elk type model.</span><span class="sxs-lookup"><span data-stu-id="73a2e-120">The modeling steps in these topics contain code that shows how to train, evaluate, save, and consume each type of model.</span></span> 

## <a name="setup-spark-clusters-and-jupyter-notebooks"></a><span data-ttu-id="73a2e-121">Instellen: De Spark-clusters en Jupyter-notebooks</span><span class="sxs-lookup"><span data-stu-id="73a2e-121">Setup: Spark clusters and Jupyter notebooks</span></span>
<span data-ttu-id="73a2e-122">Instellingsstappen en code vindt u in dit scenario voor het gebruik van een HDInsight Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="73a2e-122">Setup steps and code are provided in this walkthrough for using an HDInsight Spark 1.6.</span></span> <span data-ttu-id="73a2e-123">Maar Jupyter-notebooks zijn opgegeven voor zowel HDInsight Spark 1.6 en 2.0 Spark-clusters.</span><span class="sxs-lookup"><span data-stu-id="73a2e-123">But Jupyter notebooks are provided for both HDInsight Spark 1.6 and Spark 2.0 clusters.</span></span> <span data-ttu-id="73a2e-124">Een beschrijving van de laptops en koppelingen naar deze vindt u in de [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) voor de GitHub-opslagplaats met deze.</span><span class="sxs-lookup"><span data-stu-id="73a2e-124">A description of the notebooks and links to them are provided in the [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) for the GitHub repository containing them.</span></span> <span data-ttu-id="73a2e-125">Bovendien moet de code hier en in de gekoppelde laptops is algemeen en moet werken op een Spark-cluster.</span><span class="sxs-lookup"><span data-stu-id="73a2e-125">Moreover, the code here and in the linked notebooks is generic and should work on any Spark cluster.</span></span> <span data-ttu-id="73a2e-126">Als u HDInsight Spark niet gebruikt, is het mogelijk dat de cluster-installatie en beheer stappen enigszins afwijken van wat hier moet worden weergegeven.</span><span class="sxs-lookup"><span data-stu-id="73a2e-126">If you are not using HDInsight Spark, the cluster setup and management steps may be slightly different from what is shown here.</span></span> <span data-ttu-id="73a2e-127">Voor het gemak zijn hier de koppelingen naar de Jupyter-notebooks voor Spark 1.6 (om te worden uitgevoerd in de pySpark-kernel van de server Jupyter-Notebook) en Spark 2.0 (om te worden uitgevoerd in de kernel pySpark3 van de server Jupyter-Notebook):</span><span class="sxs-lookup"><span data-stu-id="73a2e-127">For convenience, here are the links to the Jupyter notebooks for Spark 1.6 (to be run in the pySpark kernel of the Jupyter Notebook server) and  Spark 2.0 (to be run in the pySpark3 kernel of the Jupyter Notebook server):</span></span>

### <a name="spark-16-notebooks"></a><span data-ttu-id="73a2e-128">Spark 1.6 laptops</span><span class="sxs-lookup"><span data-stu-id="73a2e-128">Spark 1.6 notebooks</span></span>
<span data-ttu-id="73a2e-129">Deze laptops zijn in de pySpark-kernel van Jupyter-notebook server moeten worden uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="73a2e-129">These notebooks are to be run in the pySpark kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="73a2e-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): bevat informatie over het uitvoeren van gegevensverkenning modelleren en batchscoreberekening met diverse verschillende algoritmen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): Provides information on how to perform data exploration, modeling, and scoring with several different algorithms.</span></span>
- <span data-ttu-id="73a2e-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): bevat onderwerpen in de notebook #1 en de ontwikkeling van het model met hyperparameter afstemmen en kruisvalidatie.</span><span class="sxs-lookup"><span data-stu-id="73a2e-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): Includes topics in notebook #1, and model development using hyperparameter tuning and cross-validation.</span></span>
- <span data-ttu-id="73a2e-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): laat zien hoe u een opgeslagen model met behulp van Python op HDInsight-clusters operationeel.</span><span class="sxs-lookup"><span data-stu-id="73a2e-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): Shows how to operationalize a saved model using Python on HDInsight clusters.</span></span>

### <a name="spark-20-notebooks"></a><span data-ttu-id="73a2e-133">Spark 2.0-laptops</span><span class="sxs-lookup"><span data-stu-id="73a2e-133">Spark 2.0 notebooks</span></span>
<span data-ttu-id="73a2e-134">Deze laptops zijn in de kernel pySpark3 van Jupyter-notebook server worden uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="73a2e-134">These notebooks are to be run in the pySpark3 kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="73a2e-135">[Spark2.0-pySpark3-machine-Learning-Data-Science-Spark-Advanced-Data-Exploration-Modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): dit bestand bevat informatie over het uitvoeren van gegevensverkenning, modelleren, en met behulp van de NYC Taxi reis scores in Spark 2.0-clusters en tarief gegevensset-beschreven [hier](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="73a2e-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): This file provides information on how to perform data exploration, modeling, and scoring in Spark 2.0 clusters using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span> <span data-ttu-id="73a2e-136">Deze laptop is mogelijk een goed uitgangspunt voor het snel verkennen van de code die we voor Spark 2.0 hebt opgegeven.</span><span class="sxs-lookup"><span data-stu-id="73a2e-136">This notebook may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> <span data-ttu-id="73a2e-137">Voor een meer gedetailleerde notebook de gegevens van de NYC Taxi analyseert, Zie de volgende notebook in deze lijst.</span><span class="sxs-lookup"><span data-stu-id="73a2e-137">For a more detailed notebook analyzes the NYC Taxi data, see the next notebook in this list.</span></span> <span data-ttu-id="73a2e-138">Zie de opmerkingen na deze lijst die deze laptops vergelijken.</span><span class="sxs-lookup"><span data-stu-id="73a2e-138">See the notes following this list that compare these notebooks.</span></span> 
- <span data-ttu-id="73a2e-139">[Spark2.0 pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): dit bestand ziet u hoe u gegevens worsteling (Spark SQL en dataframe bewerkingen), exploratie, model en score berekenen met behulp van de NYC Taxi reis en tarief set gegevens die worden beschreven [hier ](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="73a2e-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span>
- <span data-ttu-id="73a2e-140">[Spark2.0 pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): dit bestand wordt beschreven hoe gegevens worsteling (Spark SQL en dataframe bewerkingen), exploratie, model en score berekenen met behulp van de bekende luchtvaartmaatschappij tijdige afwijking uitvoeren de gegevensset van 2011 en 2012.</span><span class="sxs-lookup"><span data-stu-id="73a2e-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the well-known Airline On-time departure dataset from 2011 and 2012.</span></span> <span data-ttu-id="73a2e-141">Wij de luchtvaartmaatschappij gegevensset met de luchthaven weergegevens (bijvoorbeeld windsnelheid, temperatuur, hoogte enz.) ge√Øntegreerd v√≥√≥r modelleren, zodat deze weer-functies kunnen worden opgenomen in het model.</span><span class="sxs-lookup"><span data-stu-id="73a2e-141">We integrated the airline dataset with the airport weather data (e.g. windspeed, temperature, altitude etc.) prior to modeling, so these weather features can be included in the model.</span></span>

<!-- -->

> [!NOTE]
> <span data-ttu-id="73a2e-142">De gegevensset luchtvaartmaatschappij is toegevoegd aan de laptops Spark 2.0 ter illustratie van het gebruik van bestandsclassificatie-algoritmen beter.</span><span class="sxs-lookup"><span data-stu-id="73a2e-142">The airline dataset was added to the Spark 2.0 notebooks to better illustrate the use of classification algorithms.</span></span> <span data-ttu-id="73a2e-143">Zie de volgende koppelingen voor meer informatie over luchtvaartmaatschappij tijdige vertrek gegevensset en gegevensset weer:</span><span class="sxs-lookup"><span data-stu-id="73a2e-143">See the following links for information about airline on-time departure dataset and weather dataset:</span></span>

>- <span data-ttu-id="73a2e-144">Luchtvaartmaatschappij tijdige vertrek gegevens: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span><span class="sxs-lookup"><span data-stu-id="73a2e-144">Airline on-time departure data: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span></span>

>- <span data-ttu-id="73a2e-145">Luchthaven weergegevens: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span><span class="sxs-lookup"><span data-stu-id="73a2e-145">Airport weather data: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span></span> 
> 
> 

<!-- -->

<!-- -->

> [!NOTE]
<span data-ttu-id="73a2e-146">De notitieblokken Spark 2.0 op de NYC taxi en luchtvaartmaatschappij vlucht vertraging-gegevenssets duurt 10 minuten of langer om uit te voeren (afhankelijk van de grootte van uw HDI-cluster).</span><span class="sxs-lookup"><span data-stu-id="73a2e-146">The Spark 2.0 notebooks on the NYC taxi and airline flight delay data-sets can take 10 mins or more to run (depending on the size of your HDI cluster).</span></span> <span data-ttu-id="73a2e-147">De eerste laptop in de bovenstaande lijst geeft veel aspecten van de gegevensverkenning, visualisatie en ML-model opleiding in een laptop die het kost minder tijd om uit te voeren met een lagere actieve NYC gegevensset, waarin de bestanden taxi en tarief vooraf lid zijn: [ Spark2.0-pySpark3-machine-Learning-Data-Science-Spark-Advanced-Data-Exploration-Modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) deze notebook neemt een veel kortere tijd om te voltooien (2-3 minuten) en kan worden een goed startpunt voor de code die we hebben opgegeven snel verkennen voor Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="73a2e-147">The first notebook in the above list shows many aspects of the data exploration, visualization and ML model training in a notebook that takes less time to run with down-sampled NYC data set, in which the taxi and fare files have been pre-joined: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) This notebook takes a much shorter time to finish (2-3 mins) and may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> 

<!-- -->

<span data-ttu-id="73a2e-148">Zie voor instructies over de uitoefening van een model voor Spark 2.0 en de model-verbruik voor score berekenen, de [Spark 1.6 document over het verbruik](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) voor een voorbeeld waarin de vereiste stappen beschreven.</span><span class="sxs-lookup"><span data-stu-id="73a2e-148">For guidance on the operationalization of a Spark 2.0 model and model consumption for scoring, see the [Spark 1.6 document on consumption](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) for an example outlining the steps required.</span></span> <span data-ttu-id="73a2e-149">Om dit te gebruiken op Spark 2.0, het Python-code-bestand met vervangen [dit bestand](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span><span class="sxs-lookup"><span data-stu-id="73a2e-149">To use this on Spark 2.0, replace the Python code file with [this file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span></span>

### <a name="prerequisites"></a><span data-ttu-id="73a2e-150">Vereisten</span><span class="sxs-lookup"><span data-stu-id="73a2e-150">Prerequisites</span></span>
<span data-ttu-id="73a2e-151">De volgende procedures hebben betrekking op Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="73a2e-151">The following procedures are related to Spark 1.6.</span></span> <span data-ttu-id="73a2e-152">Gebruik de laptops beschreven en gekoppeld aan de eerder voor de versie 2.0 van Spark.</span><span class="sxs-lookup"><span data-stu-id="73a2e-152">For  the Spark 2.0 version, use the notebooks described and linked to previously.</span></span> 

<span data-ttu-id="73a2e-153">1. u moet een Azure-abonnement hebben.</span><span class="sxs-lookup"><span data-stu-id="73a2e-153">1.You must have an Azure subscription.</span></span> <span data-ttu-id="73a2e-154">Als u nog geen een, Zie [gratis proefversie van Azure ophalen](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span><span class="sxs-lookup"><span data-stu-id="73a2e-154">If you do not already have one, see [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>

<span data-ttu-id="73a2e-155">2. u moet een 1.6 Spark-cluster voor dit scenario.</span><span class="sxs-lookup"><span data-stu-id="73a2e-155">2.You need a Spark 1.6 cluster to complete this walkthrough.</span></span> <span data-ttu-id="73a2e-156">Zie de instructies in om een [aan de slag: maken van Apache Spark in Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="73a2e-156">To create one, see the instructions provided in [Get started: create Apache Spark on Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span></span> <span data-ttu-id="73a2e-157">Het clustertype en de versie is opgegeven in de **clustertype Selecteer** menu.</span><span class="sxs-lookup"><span data-stu-id="73a2e-157">The cluster type and version is specified from the **Select Cluster Type** menu.</span></span> 

![Cluster configureren](./media/machine-learning-data-science-spark-overview/spark-cluster-on-portal.png)

<!-- -->

> [!NOTE]
> <span data-ttu-id="73a2e-159">Zie voor een onderwerp waarin wordt uitgelegd hoe u Scala in plaats van Python gebruikt om taken voor een end-to-end gegevens wetenschap proces te voltooien, de [Gegevenswetenschap met Spark op Azure met behulp van Scala](machine-learning-data-science-process-scala-walkthrough.md).</span><span class="sxs-lookup"><span data-stu-id="73a2e-159">For a topic that shows how to use Scala rather than Python to complete tasks for an end-to-end data science process, see the [Data Science using Scala with Spark on Azure](machine-learning-data-science-process-scala-walkthrough.md).</span></span>
> 
> 

<!-- -->

> [!INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]
> 
> 

## <a name="the-nyc-2013-taxi-data"></a><span data-ttu-id="73a2e-160">De gegevens van de NYC 2013 Taxi</span><span class="sxs-lookup"><span data-stu-id="73a2e-160">The NYC 2013 Taxi data</span></span>
<span data-ttu-id="73a2e-161">De gegevens van de NYC Taxi reis is ongeveer 20 GB gecomprimeerde door komma's gescheiden waarden (CSV)-bestanden (~ 48 GB ongecomprimeerde), die bestaat uit meer dan 173 miljoen afzonderlijke reizen en de tarieven voor elke reis betaald.</span><span class="sxs-lookup"><span data-stu-id="73a2e-161">The NYC Taxi Trip data is about 20 GB of compressed comma-separated values (CSV) files (~48 GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</span></span> <span data-ttu-id="73a2e-162">Elke record reis omvat de ophalen en afgiftepunt en tijd, geanonimiseerde hack (van stuurprogramma) licentienummer, en nummer straten (taxi van unieke id).</span><span class="sxs-lookup"><span data-stu-id="73a2e-162">Each trip record includes the pick up and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi‚Äôs unique id) number.</span></span> <span data-ttu-id="73a2e-163">De gegevens bevat informatie over alle reizen in het jaar 2013 en is beschikbaar in de volgende twee gegevenssets voor elke maand:</span><span class="sxs-lookup"><span data-stu-id="73a2e-163">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</span></span>

1. <span data-ttu-id="73a2e-164">De CSV-bestanden 'trip_data' reis details, zoals het aantal passagiers bevatten, kunnen worden opgepikt en dropoff verwijst, krachtvoertuigen duur en reis lengte.</span><span class="sxs-lookup"><span data-stu-id="73a2e-164">The 'trip_data' CSV files contain trip details, such as number of passengers, pick up and dropoff points, trip duration, and trip length.</span></span> <span data-ttu-id="73a2e-165">Hier volgen enkele voorbeeldrecords:</span><span class="sxs-lookup"><span data-stu-id="73a2e-165">Here are a few sample records:</span></span>
   
        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868
2. <span data-ttu-id="73a2e-166">De 'trip_fare' CSV-bestanden bevatten details van het tarief dat betaald voor elke reis, zoals betalingstype, tarief bedrag, extra kosten en belastingen, tips en tolgelden, en de totale hoeveelheid betaald.</span><span class="sxs-lookup"><span data-stu-id="73a2e-166">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</span></span> <span data-ttu-id="73a2e-167">Hier volgen enkele voorbeeldrecords:</span><span class="sxs-lookup"><span data-stu-id="73a2e-167">Here are a few sample records:</span></span>
   
        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5

<span data-ttu-id="73a2e-168">We hebben een steekproef 0,1% van deze bestanden gemaakt en die lid zijn van de reis\_gegevens en reis\_ritbedrag CSV-bestanden in een enkel gegevensset wilt gebruiken als de invoergegevensset voor dit scenario.</span><span class="sxs-lookup"><span data-stu-id="73a2e-168">We have taken a 0.1% sample of these files and joined the trip\_data and trip\_fare CVS files into a single dataset to use as the input dataset for this walkthrough.</span></span> <span data-ttu-id="73a2e-169">De unieke sleutel toevoegen reis\_gegevens en reis\_tarief dat bestaat uit de velden: straten, hack\_en ophalen van certificaat\_datetime.</span><span class="sxs-lookup"><span data-stu-id="73a2e-169">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</span></span> <span data-ttu-id="73a2e-170">Elke record van de gegevensset bevat de volgende kenmerken die een reis NYC Taxi vertegenwoordigt:</span><span class="sxs-lookup"><span data-stu-id="73a2e-170">Each record of the dataset contains the following attributes representing a NYC Taxi trip:</span></span>

| <span data-ttu-id="73a2e-171">Veld</span><span class="sxs-lookup"><span data-stu-id="73a2e-171">Field</span></span> | <span data-ttu-id="73a2e-172">Korte beschrijving</span><span class="sxs-lookup"><span data-stu-id="73a2e-172">Brief Description</span></span> |
| --- | --- |
| <span data-ttu-id="73a2e-173">straten</span><span class="sxs-lookup"><span data-stu-id="73a2e-173">medallion</span></span> |<span data-ttu-id="73a2e-174">Geanonimiseerde taxi straten (taxi unieke id)</span><span class="sxs-lookup"><span data-stu-id="73a2e-174">Anonymized taxi medallion (unique taxi id)</span></span> |
| <span data-ttu-id="73a2e-175">hack_license</span><span class="sxs-lookup"><span data-stu-id="73a2e-175">hack_license</span></span> |<span data-ttu-id="73a2e-176">Geanonimiseerde Hackney regeleinde licentienummer</span><span class="sxs-lookup"><span data-stu-id="73a2e-176">Anonymized Hackney Carriage License number</span></span> |
| <span data-ttu-id="73a2e-177">vendor_id</span><span class="sxs-lookup"><span data-stu-id="73a2e-177">vendor_id</span></span> |<span data-ttu-id="73a2e-178">Taxi leveranciers-id</span><span class="sxs-lookup"><span data-stu-id="73a2e-178">Taxi vendor id</span></span> |
| <span data-ttu-id="73a2e-179">rate_code</span><span class="sxs-lookup"><span data-stu-id="73a2e-179">rate_code</span></span> |<span data-ttu-id="73a2e-180">Snelheid van de NYC taxi van tarief</span><span class="sxs-lookup"><span data-stu-id="73a2e-180">NYC taxi rate of fare</span></span> |
| <span data-ttu-id="73a2e-181">store_and_fwd_flag</span><span class="sxs-lookup"><span data-stu-id="73a2e-181">store_and_fwd_flag</span></span> |<span data-ttu-id="73a2e-182">Store en voorwaarts markering</span><span class="sxs-lookup"><span data-stu-id="73a2e-182">Store and forward flag</span></span> |
| <span data-ttu-id="73a2e-183">pickup_datetime</span><span class="sxs-lookup"><span data-stu-id="73a2e-183">pickup_datetime</span></span> |<span data-ttu-id="73a2e-184">Datum en tijd kunnen worden opgepikt</span><span class="sxs-lookup"><span data-stu-id="73a2e-184">Pick up date & time</span></span> |
| <span data-ttu-id="73a2e-185">dropoff_datetime</span><span class="sxs-lookup"><span data-stu-id="73a2e-185">dropoff_datetime</span></span> |<span data-ttu-id="73a2e-186">Dropoff datum en tijd</span><span class="sxs-lookup"><span data-stu-id="73a2e-186">Dropoff date & time</span></span> |
| <span data-ttu-id="73a2e-187">pickup_hour</span><span class="sxs-lookup"><span data-stu-id="73a2e-187">pickup_hour</span></span> |<span data-ttu-id="73a2e-188">Uur ophalen</span><span class="sxs-lookup"><span data-stu-id="73a2e-188">Pick up hour</span></span> |
| <span data-ttu-id="73a2e-189">pickup_week</span><span class="sxs-lookup"><span data-stu-id="73a2e-189">pickup_week</span></span> |<span data-ttu-id="73a2e-190">Week van het jaar ophalen</span><span class="sxs-lookup"><span data-stu-id="73a2e-190">Pick up week of the year</span></span> |
| <span data-ttu-id="73a2e-191">werkdag</span><span class="sxs-lookup"><span data-stu-id="73a2e-191">weekday</span></span> |<span data-ttu-id="73a2e-192">Werkdag (bereik 1-7)</span><span class="sxs-lookup"><span data-stu-id="73a2e-192">Weekday (range 1-7)</span></span> |
| <span data-ttu-id="73a2e-193">passenger_count</span><span class="sxs-lookup"><span data-stu-id="73a2e-193">passenger_count</span></span> |<span data-ttu-id="73a2e-194">Aantal passagiers in een reis taxi</span><span class="sxs-lookup"><span data-stu-id="73a2e-194">Number of passengers in a taxi trip</span></span> |
| <span data-ttu-id="73a2e-195">trip_time_in_secs</span><span class="sxs-lookup"><span data-stu-id="73a2e-195">trip_time_in_secs</span></span> |<span data-ttu-id="73a2e-196">Reis tijd in seconden</span><span class="sxs-lookup"><span data-stu-id="73a2e-196">Trip time in seconds</span></span> |
| <span data-ttu-id="73a2e-197">trip_distance</span><span class="sxs-lookup"><span data-stu-id="73a2e-197">trip_distance</span></span> |<span data-ttu-id="73a2e-198">Reis afstand afgelegd in mijl</span><span class="sxs-lookup"><span data-stu-id="73a2e-198">Trip distance traveled in miles</span></span> |
| <span data-ttu-id="73a2e-199">pickup_longitude</span><span class="sxs-lookup"><span data-stu-id="73a2e-199">pickup_longitude</span></span> |<span data-ttu-id="73a2e-200">Lengtegraad ophalen</span><span class="sxs-lookup"><span data-stu-id="73a2e-200">Pick up longitude</span></span> |
| <span data-ttu-id="73a2e-201">pickup_latitude</span><span class="sxs-lookup"><span data-stu-id="73a2e-201">pickup_latitude</span></span> |<span data-ttu-id="73a2e-202">Breedtegraad ophalen</span><span class="sxs-lookup"><span data-stu-id="73a2e-202">Pick up latitude</span></span> |
| <span data-ttu-id="73a2e-203">dropoff_longitude</span><span class="sxs-lookup"><span data-stu-id="73a2e-203">dropoff_longitude</span></span> |<span data-ttu-id="73a2e-204">Lengtegraad Dropoff</span><span class="sxs-lookup"><span data-stu-id="73a2e-204">Dropoff longitude</span></span> |
| <span data-ttu-id="73a2e-205">dropoff_latitude</span><span class="sxs-lookup"><span data-stu-id="73a2e-205">dropoff_latitude</span></span> |<span data-ttu-id="73a2e-206">Dropoff breedtegraad</span><span class="sxs-lookup"><span data-stu-id="73a2e-206">Dropoff latitude</span></span> |
| <span data-ttu-id="73a2e-207">direct_distance</span><span class="sxs-lookup"><span data-stu-id="73a2e-207">direct_distance</span></span> |<span data-ttu-id="73a2e-208">Afstand tussen pick up directe en dropoff locaties</span><span class="sxs-lookup"><span data-stu-id="73a2e-208">Direct distance between pick up and dropoff locations</span></span> |
| <span data-ttu-id="73a2e-209">payment_type</span><span class="sxs-lookup"><span data-stu-id="73a2e-209">payment_type</span></span> |<span data-ttu-id="73a2e-210">Betalingstype (CA's, creditcard enz.)</span><span class="sxs-lookup"><span data-stu-id="73a2e-210">Payment type (cas, credit-card etc.)</span></span> |
| <span data-ttu-id="73a2e-211">fare_amount</span><span class="sxs-lookup"><span data-stu-id="73a2e-211">fare_amount</span></span> |<span data-ttu-id="73a2e-212">Tarief bedrag in</span><span class="sxs-lookup"><span data-stu-id="73a2e-212">Fare amount in</span></span> |
| <span data-ttu-id="73a2e-213">Extra kosten</span><span class="sxs-lookup"><span data-stu-id="73a2e-213">surcharge</span></span> |<span data-ttu-id="73a2e-214">Extra kosten</span><span class="sxs-lookup"><span data-stu-id="73a2e-214">Surcharge</span></span> |
| <span data-ttu-id="73a2e-215">mta_tax</span><span class="sxs-lookup"><span data-stu-id="73a2e-215">mta_tax</span></span> |<span data-ttu-id="73a2e-216">MTA belasting</span><span class="sxs-lookup"><span data-stu-id="73a2e-216">Mta tax</span></span> |
| <span data-ttu-id="73a2e-217">tip_amount</span><span class="sxs-lookup"><span data-stu-id="73a2e-217">tip_amount</span></span> |<span data-ttu-id="73a2e-218">Tip bedrag</span><span class="sxs-lookup"><span data-stu-id="73a2e-218">Tip amount</span></span> |
| <span data-ttu-id="73a2e-219">tolls_amount</span><span class="sxs-lookup"><span data-stu-id="73a2e-219">tolls_amount</span></span> |<span data-ttu-id="73a2e-220">Tolgelden bedrag</span><span class="sxs-lookup"><span data-stu-id="73a2e-220">Tolls amount</span></span> |
| <span data-ttu-id="73a2e-221">total_amount</span><span class="sxs-lookup"><span data-stu-id="73a2e-221">total_amount</span></span> |<span data-ttu-id="73a2e-222">Totale hoeveelheid</span><span class="sxs-lookup"><span data-stu-id="73a2e-222">Total amount</span></span> |
| <span data-ttu-id="73a2e-223">punt</span><span class="sxs-lookup"><span data-stu-id="73a2e-223">tipped</span></span> |<span data-ttu-id="73a2e-224">Gekantelde (0/1 voor Nee of Ja)</span><span class="sxs-lookup"><span data-stu-id="73a2e-224">Tipped (0/1 for no or yes)</span></span> |
| <span data-ttu-id="73a2e-225">tip_class</span><span class="sxs-lookup"><span data-stu-id="73a2e-225">tip_class</span></span> |<span data-ttu-id="73a2e-226">Tip klasse (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span><span class="sxs-lookup"><span data-stu-id="73a2e-226">Tip class (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span></span> |

## <a name="execute-code-from-a-jupyter-notebook-on-the-spark-cluster"></a><span data-ttu-id="73a2e-227">Voer de code van een Jupyter-notebook in Spark-cluster</span><span class="sxs-lookup"><span data-stu-id="73a2e-227">Execute code from a Jupyter notebook on the Spark cluster</span></span>
<span data-ttu-id="73a2e-228">U kunt de Jupyter-Notebook vanuit de Azure-portal op te starten.</span><span class="sxs-lookup"><span data-stu-id="73a2e-228">You can launch the Jupyter Notebook from the Azure portal.</span></span> <span data-ttu-id="73a2e-229">Zoek uw Spark-cluster op uw dashboard en klik op de beheerpagina invoeren voor uw cluster.</span><span class="sxs-lookup"><span data-stu-id="73a2e-229">Find your Spark cluster on your dashboard and click it to enter management page for your cluster.</span></span> <span data-ttu-id="73a2e-230">De notebook die zijn gekoppeld aan het Spark-cluster, klikt u op **Clusterdashboards** -> **Jupyter-Notebook** .</span><span class="sxs-lookup"><span data-stu-id="73a2e-230">To open the notebook associated with the Spark cluster, click **Cluster Dashboards** -> **Jupyter Notebook** .</span></span>

![Clusterdashboards](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

<span data-ttu-id="73a2e-232">U kunt ook bladeren naar ***https://CLUSTERNAME.azurehdinsight.NET/jupyter*** voor toegang tot de Jupyter-Notebooks.</span><span class="sxs-lookup"><span data-stu-id="73a2e-232">You can also browse to ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** to access the Jupyter Notebooks.</span></span> <span data-ttu-id="73a2e-233">De CLUSTERNAAM deel uitmaakt van deze URL vervangen door de naam van uw eigen cluster.</span><span class="sxs-lookup"><span data-stu-id="73a2e-233">Replace the CLUSTERNAME part of this URL with the name of your own cluster.</span></span> <span data-ttu-id="73a2e-234">U moet het wachtwoord voor uw beheerdersaccount voor toegang tot de laptops.</span><span class="sxs-lookup"><span data-stu-id="73a2e-234">You need the password for your admin account to access the notebooks.</span></span>

![Jupyter-Notebooks bladeren](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

<span data-ttu-id="73a2e-236">Selecteer PySpark om te zien van een map met een paar voorbeelden van vooraf verpakte laptops die gebruikmaken van de PySpark-API. De laptops die de codevoorbeelden voor dit pakket van Spark onderwerp bevatten zijn beschikbaar op [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span><span class="sxs-lookup"><span data-stu-id="73a2e-236">Select PySpark to see a directory that contains a few examples of pre-packaged notebooks that use the PySpark API.The notebooks that contain the code samples for this suite of Spark topic are available at [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span></span>

<span data-ttu-id="73a2e-237">U kunt de laptops rechtstreeks vanuit uploaden [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) naar de server Jupyter-notebook in Spark-cluster.</span><span class="sxs-lookup"><span data-stu-id="73a2e-237">You can upload the notebooks directly from [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) to the Jupyter notebook server on your Spark cluster.</span></span> <span data-ttu-id="73a2e-238">Klik op de startpagina van uw Jupyter op de **uploaden** knop op de juiste deel van het scherm.</span><span class="sxs-lookup"><span data-stu-id="73a2e-238">On the home page of your Jupyter, click the **Upload** button on the right part of the screen.</span></span> <span data-ttu-id="73a2e-239">Hiermee opent u een Windows Verkenner.</span><span class="sxs-lookup"><span data-stu-id="73a2e-239">It opens a file explorer.</span></span> <span data-ttu-id="73a2e-240">Hier kunt u de URL GitHub (onbewerkte inhoud) van de Notebook en klik op Plakken **Open**.</span><span class="sxs-lookup"><span data-stu-id="73a2e-240">Here you can paste the GitHub (raw content) URL of the Notebook and click **Open**.</span></span> 

<span data-ttu-id="73a2e-241">U ziet de bestandsnaam in de lijst met Jupyter-bestand met een **uploaden** knop opnieuw.</span><span class="sxs-lookup"><span data-stu-id="73a2e-241">You see the file name on your Jupyter file list with an **Upload** button again.</span></span> <span data-ttu-id="73a2e-242">Klik hierop **uploaden** knop.</span><span class="sxs-lookup"><span data-stu-id="73a2e-242">Click this **Upload** button.</span></span> <span data-ttu-id="73a2e-243">Nu hebt u de notebook ge√Ømporteerd.</span><span class="sxs-lookup"><span data-stu-id="73a2e-243">Now you have imported the notebook.</span></span> <span data-ttu-id="73a2e-244">Herhaal deze stappen voor het uploaden van de andere notitieblokken van dit scenario.</span><span class="sxs-lookup"><span data-stu-id="73a2e-244">Repeat these steps to upload the other notebooks from this walkthrough.</span></span>

> [!TIP]
> <span data-ttu-id="73a2e-245">U kunt met de rechtermuisknop op de koppelingen in de browser en selecteer **koppeling kopi√´ren** om de URL github onbewerkte inhoud te krijgen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-245">You can right-click the links on your browser and select **Copy Link** to get the github raw content URL.</span></span> <span data-ttu-id="73a2e-246">U kunt deze URL in het uploaden van Jupyter explorer bestandsdialoogvenster plakken.</span><span class="sxs-lookup"><span data-stu-id="73a2e-246">You can paste this URL into the Jupyter Upload file explorer dialog box.</span></span>
> 
> 

<span data-ttu-id="73a2e-247">U kunt nu:</span><span class="sxs-lookup"><span data-stu-id="73a2e-247">Now you can:</span></span>

* <span data-ttu-id="73a2e-248">De code door te klikken op de notebook bekijken.</span><span class="sxs-lookup"><span data-stu-id="73a2e-248">See the code by clicking the notebook.</span></span>
* <span data-ttu-id="73a2e-249">elke cel uitvoeren door te drukken **SHIFT + ENTER**.</span><span class="sxs-lookup"><span data-stu-id="73a2e-249">Execute each cell by pressing **SHIFT-ENTER**.</span></span>
* <span data-ttu-id="73a2e-250">De volledige notebook uitvoeren door te klikken op **cel** -> **uitvoeren**.</span><span class="sxs-lookup"><span data-stu-id="73a2e-250">Run the entire notebook by clicking on **Cell** -> **Run**.</span></span>
* <span data-ttu-id="73a2e-251">Gebruik de automatische visualisatie van query's.</span><span class="sxs-lookup"><span data-stu-id="73a2e-251">Use the automatic visualization of queries.</span></span>

> [!TIP]
> <span data-ttu-id="73a2e-252">De PySpark-kernel visualiseren automatisch de uitvoer van SQL (HiveQL)-query's.</span><span class="sxs-lookup"><span data-stu-id="73a2e-252">The PySpark kernel automatically visualizes the output of SQL (HiveQL) queries.</span></span> <span data-ttu-id="73a2e-253">Krijgt u de optie kiezen uit verschillende soorten visualisaties (tabel-, cirkel, regel, gebied of balk) met behulp van de **Type** menuknoppen in de notebook:</span><span class="sxs-lookup"><span data-stu-id="73a2e-253">You are given the option to select among several different types of visualizations (Table, Pie, Line, Area, or Bar) by using the **Type** menu buttons in the notebook:</span></span>
> 
> 

![Logistic regression ROC-curve voor algemene methode](./media/machine-learning-data-science-spark-overview/pyspark-jupyter-autovisualization.png)

## <a name="whats-next"></a><span data-ttu-id="73a2e-255">Volgende stappen</span><span class="sxs-lookup"><span data-stu-id="73a2e-255">What's next?</span></span>
<span data-ttu-id="73a2e-256">Nu dat u een HDInsight Spark-cluster is ingesteld en de Jupyter-notebooks met licentie zijn ge√ºpload, bent u klaar om te werken via de onderwerpen die met de drie PySpark laptops overeenkomen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-256">Now that you are set up with an HDInsight Spark cluster and have uploaded the Jupyter notebooks, you are ready to work through the topics that correspond to the three PySpark notebooks.</span></span> <span data-ttu-id="73a2e-257">Ze geven weer hoe Verken uw gegevens en het maken en gebruiken van modellen.</span><span class="sxs-lookup"><span data-stu-id="73a2e-257">They show how to explore your data and then how to create and consume models.</span></span> <span data-ttu-id="73a2e-258">De geavanceerde gegevens te verkennen en modellering notebook laat zien hoe naar kruisvalidatie, hyper-parameter verstrekkende, opnemen en model van de evaluatie.</span><span class="sxs-lookup"><span data-stu-id="73a2e-258">The advanced data exploration and modeling notebook shows how to include cross-validation, hyper-parameter sweeping, and model evaluation.</span></span> 

<span data-ttu-id="73a2e-259">**Gegevensverkenning en modellering met Spark:** verkennen van de gegevensset en maken, beoordelen en evalueren van de machine learning-modellen doorloopt de [binaire classificatie en regressie modellen voor gegevens met de MLlib Spark maken Toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) onderwerp.</span><span class="sxs-lookup"><span data-stu-id="73a2e-259">**Data Exploration and modeling with Spark:** Explore the dataset and create, score, and evaluate the machine learning models by working through the [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) topic.</span></span>

<span data-ttu-id="73a2e-260">**Model verbruik:** Zie voor meer informatie over de classificatie en regressie modellen gemaakt in dit onderwerp te beoordelen, [Score en evalueren van Spark is gebouwd machine learning-modellen](machine-learning-data-science-spark-model-consumption.md).</span><span class="sxs-lookup"><span data-stu-id="73a2e-260">**Model consumption:** To learn how to score the classification and regression models created in this topic, see [Score and evaluate Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md).</span></span>

<span data-ttu-id="73a2e-261">**Kruisvalidatie en hyperparameter sweeping**: Zie [geavanceerde gegevensverkenning en modellering met Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) over de manier waarop modellen kunnen getraind met behulp van kruisvalidatie en hyper-parameter sweeping</span><span class="sxs-lookup"><span data-stu-id="73a2e-261">**Cross-validation and hyperparameter sweeping**: See [Advanced data exploration and modeling with Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) on how models can be trained using cross-validation and hyper-parameter sweeping</span></span>


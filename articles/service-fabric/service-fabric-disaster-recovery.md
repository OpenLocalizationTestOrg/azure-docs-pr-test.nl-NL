---
title: Service Fabric-noodherstel aaaAzure | Microsoft Docs
description: Azure Service Fabric biedt Hallo mogelijkheden nodig toodeal met alle soorten noodsituaties. Dit artikel wordt beschreven Hallo typen noodsituaties die zich kunnen voordoen en hoe toodeal mee.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="44dcc-104">Herstel na noodgevallen in Azure Service Fabric</span><span class="sxs-lookup"><span data-stu-id="44dcc-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="44dcc-105">Een belangrijk onderdeel van het leveren van hoge beschikbaarheid is ervoor te zorgen dat alle soorten fouten meer services kunnen doorstaan.</span><span class="sxs-lookup"><span data-stu-id="44dcc-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="44dcc-106">Dit is vooral belangrijk voor fouten die niet-geplande zijn en buiten uw beheer.</span><span class="sxs-lookup"><span data-stu-id="44dcc-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="44dcc-107">Dit artikel worden enkele algemene fout modi die mogelijk noodsituaties als dat niet het gemodelleerd en correct worden beheerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="44dcc-108">Deze oplossingen en acties tootake Bespreek ook als er een ramp toch opgetreden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-108">It also discuss mitigations and actions tootake if a disaster happened anyway.</span></span> <span data-ttu-id="44dcc-109">Hallo-doel is toolimit of Hallo risico uitvaltijd of gegevensverlies te voorkomen wanneer ze, fouten optreden, gepland of anders optreden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-109">hello goal is toolimit or eliminate hello risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="44dcc-110">Noodherstel voorkomen</span><span class="sxs-lookup"><span data-stu-id="44dcc-110">Avoiding disaster</span></span>
<span data-ttu-id="44dcc-111">Het voornaamste doel van de service Fabric is toohelp u zowel uw omgeving en uw services zodanig dat algemene fout typen niet noodsituaties zijn model.</span><span class="sxs-lookup"><span data-stu-id="44dcc-111">Service Fabric's primary goal is toohelp you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="44dcc-112">In het algemeen zijn er twee soorten na noodgevallen/mislukte scenario's:</span><span class="sxs-lookup"><span data-stu-id="44dcc-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="44dcc-113">Hardware of software-fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-113">Hardware or software faults</span></span>
2. <span data-ttu-id="44dcc-114">Operationele fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="44dcc-115">Hardware en software-fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-115">Hardware and software faults</span></span>
<span data-ttu-id="44dcc-116">Hardware en software fouten zijn onvoorspelbaar.</span><span class="sxs-lookup"><span data-stu-id="44dcc-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="44dcc-117">Hallo gemakkelijkste manier toosurvive fouten kan meer exemplaren van Hallo service omspannen buiten de grenzen van de fout hardware of software wordt uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-117">hello easiest way toosurvive faults is running more copies of hello service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="44dcc-118">Bijvoorbeeld, als uw service alleen op een bepaalde computer wordt uitgevoerd, is klikt u vervolgens hello mislukte van die één machine een noodgeval voor de service.</span><span class="sxs-lookup"><span data-stu-id="44dcc-118">For example, if your service is running only on one particular machine, then hello failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="44dcc-119">Hallo op eenvoudige wijze tooavoid deze noodherstel is tooensure die Hallo service daadwerkelijk wordt uitgevoerd op meerdere machines.</span><span class="sxs-lookup"><span data-stu-id="44dcc-119">hello simple way tooavoid this disaster is tooensure that hello service is actually running on multiple machines.</span></span> <span data-ttu-id="44dcc-120">Testen is ook nodig tooensure Hallo uitvallen van één machine Hallo waarop service wordt uitgevoerd niet verstoren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-120">Testing is also necessary tooensure hello failure of one machine doesn't disrupt hello running service.</span></span> <span data-ttu-id="44dcc-121">Plannen van capaciteit zorgt ervoor dat het exemplaar van een vervanging kan worden gemaakt elders en dat vermindering van de capaciteit Hallo resterende services biedt geen overbelasting.</span><span class="sxs-lookup"><span data-stu-id="44dcc-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload hello remaining services.</span></span> <span data-ttu-id="44dcc-122">Hallo hetzelfde patroon werkt ongeacht wat u tooavoid Hallo mislukt probeert.</span><span class="sxs-lookup"><span data-stu-id="44dcc-122">hello same pattern works regardless of what you're trying tooavoid hello failure of.</span></span> <span data-ttu-id="44dcc-123">Bijvoorbeeld.</span><span class="sxs-lookup"><span data-stu-id="44dcc-123">For example.</span></span> <span data-ttu-id="44dcc-124">Als u zich zorgen over Hallo falen van een SAN maakt, loopt u over meerdere SAN's.</span><span class="sxs-lookup"><span data-stu-id="44dcc-124">if you're concerned about hello failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="44dcc-125">Als u zich zorgen over Hallo verlies van een rek met servers maakt, voert u via meerdere rekken.</span><span class="sxs-lookup"><span data-stu-id="44dcc-125">If you're concerned about hello loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="44dcc-126">Als u Hallo verlies van datacenters twijfelt, moet uw service uitvoeren op meerdere Azure-regio's of datacenters.</span><span class="sxs-lookup"><span data-stu-id="44dcc-126">If you're worried about hello loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="44dcc-127">Wanneer in deze modus wordt uitgevoerd, u nog steeds onderwerp toosome typen gelijktijdige storingen, maar één en zelfs meerdere van een bepaald type (bijvoorbeeld: een enkele virtuele machine of netwerk koppeling mislukken) automatisch worden verwerkt (en dus niet langer een 'noodgeval').</span><span class="sxs-lookup"><span data-stu-id="44dcc-127">When running in this type of spanned mode, you're still subject toosome types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="44dcc-128">Service Fabric bevat veel mechanismen voor het uitbreiden van Hallo-cluster en ingangen knooppunten en services terug te brengen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-128">Service Fabric provides many mechanisms for expanding hello cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="44dcc-129">Service Fabric kunnen ook met veel exemplaren van uw services in volgorde tooavoid deze typen niet-geplande storingen uit te schakelen in echte noodsituaties.</span><span class="sxs-lookup"><span data-stu-id="44dcc-129">Service Fabric also allows running many instances of your services in order tooavoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="44dcc-130">Mogelijk zijn er redenen waarom een groot genoeg toospan implementatie uitgevoerd gedurende fouten niet haalbaar is.</span><span class="sxs-lookup"><span data-stu-id="44dcc-130">There may be reasons why running a deployment large enough toospan over failures is not feasible.</span></span> <span data-ttu-id="44dcc-131">Bijvoorbeeld, duurt het meer hardwarebronnen dan je niet bereid toopay voor relatieve toohello kans van de fout.</span><span class="sxs-lookup"><span data-stu-id="44dcc-131">For example, it may take more hardware resources than you're not willing toopay for relative toohello chance of failure.</span></span> <span data-ttu-id="44dcc-132">Tijdens het afhandelen van gedistribueerde toepassingen, kan het zijn dat aanvullende communicatie hops of status replicatie over geografische afstanden oorzaken onaanvaardbaar latentie kost.</span><span class="sxs-lookup"><span data-stu-id="44dcc-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="44dcc-133">Waar deze regel wordt getekend verschilt voor elke toepassing.</span><span class="sxs-lookup"><span data-stu-id="44dcc-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="44dcc-134">Voor softwarefouten in het bijzonder Hallo veroorzaakt mogelijk Hallo service dat u tooscale probeert.</span><span class="sxs-lookup"><span data-stu-id="44dcc-134">For software faults specifically, hello fault could be in hello service that you are trying tooscale.</span></span> <span data-ttu-id="44dcc-135">In dit geval voorkomen niet meer exemplaren Hallo na noodgevallen, omdat Hallo fouttoestand worden gecorreleerd over alle Hallo-exemplaren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-135">In this case more copies don't prevent hello disaster, since hello failure condition is correlated across all hello instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="44dcc-136">Operationele fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-136">Operational faults</span></span>
<span data-ttu-id="44dcc-137">Zelfs als uw service Hallo wereld met veel redundantie upset, kan deze nog steeds rampzalig zijn gebeurtenissen optreden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-137">Even if your service is spanned across hello globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="44dcc-138">Bijvoorbeeld als iemand per ongeluk Hallo DNS-naam voor de service Hallo geconfigureerd of deze rechtstreekse wordt verwijderd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-138">For example, if someone accidentally reconfigures hello dns name for hello service, or deletes it outright.</span></span> <span data-ttu-id="44dcc-139">Stel bijvoorbeeld moest u een stateful Service Fabric-service en iemand die service per ongeluk hebt verwijderd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="44dcc-140">Tenzij er een aantal andere risicobeperking, of de service en alle Hallo status die deze had is nu voltooid.</span><span class="sxs-lookup"><span data-stu-id="44dcc-140">Unless there's some other mitigation, that service and all of hello state it had is now gone.</span></span> <span data-ttu-id="44dcc-141">Dit type operationele noodsituaties vereist ('Oeps') verschillende oplossingen en stappen voor herstel dan gewone niet-geplande storingen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="44dcc-142">Hallo aanbevolen manieren tooavoid deze typen operationele storingen zijn aan</span><span class="sxs-lookup"><span data-stu-id="44dcc-142">hello best ways tooavoid these types of operational faults are to</span></span>
1. <span data-ttu-id="44dcc-143">operationele toegang toohello omgeving beperken</span><span class="sxs-lookup"><span data-stu-id="44dcc-143">restrict operational access toohello environment</span></span>
2. <span data-ttu-id="44dcc-144">strikt gevaarlijke bewerkingen controleren</span><span class="sxs-lookup"><span data-stu-id="44dcc-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="44dcc-145">automation leggen, te voorkomen dat handmatige of buiten band wijzigingen en specifieke wijzigingen tegen Hallo werkelijke omgeving valideren voordat ze vast te stellen</span><span class="sxs-lookup"><span data-stu-id="44dcc-145">impose automation, prevent manual or out of band changes, and validate specific changes against hello actual environment before enacting them</span></span>
4. <span data-ttu-id="44dcc-146">Zorg ervoor dat destructieve operations 'soft'.</span><span class="sxs-lookup"><span data-stu-id="44dcc-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="44dcc-147">Voorlopig bewerkingen onmiddellijk van kracht niet of kunnen in sommige tijdvenster ongedaan worden gemaakt</span><span class="sxs-lookup"><span data-stu-id="44dcc-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="44dcc-148">Service Fabric bevat enkele mechanismen tooprevent operationele fouten, zoals bieden [op basis van rollen](service-fabric-cluster-security-roles.md) toegangsbeheer voor bewerkingen voor een cluster.</span><span class="sxs-lookup"><span data-stu-id="44dcc-148">Service Fabric provides some mechanisms tooprevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="44dcc-149">De meeste van deze operationele storingen vereisen echter organisatie inspanningen en andere systemen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="44dcc-150">Service Fabric bieden een mechanisme voor functionerende operationele fouten, met name back-up en herstel voor stateful services.</span><span class="sxs-lookup"><span data-stu-id="44dcc-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="44dcc-151">Het beheren van fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-151">Managing failures</span></span>
<span data-ttu-id="44dcc-152">Hallo-doel van de Service Fabric is bijna altijd automatisch beheer van fouten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-152">hello goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="44dcc-153">Echter, in volgorde toohandle bepaalde typen fouten, services moeten aanvullende code hebben.</span><span class="sxs-lookup"><span data-stu-id="44dcc-153">However, in order toohandle some types of failures, services must have additional code.</span></span> <span data-ttu-id="44dcc-154">Andere typen van fouten moeten _niet_ worden automatisch opgelost redenen veiligheid en zakelijke continuïteit.</span><span class="sxs-lookup"><span data-stu-id="44dcc-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="44dcc-155">Afhandeling van één fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-155">Handling single failures</span></span>
<span data-ttu-id="44dcc-156">Meerdere computers kunnen om allerlei redenen mislukken.</span><span class="sxs-lookup"><span data-stu-id="44dcc-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="44dcc-157">Sommige van deze zijn oorzaken van hardware, zoals voedingen en netwerken hardwarefouten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="44dcc-158">Andere fouten zijn in software.</span><span class="sxs-lookup"><span data-stu-id="44dcc-158">Other failures are in software.</span></span> <span data-ttu-id="44dcc-159">Het gaat hierbij om fouten van Hallo van het besturingssysteem en het Hallo-service zelf.</span><span class="sxs-lookup"><span data-stu-id="44dcc-159">These include failures of hello actual operating system and hello service itself.</span></span> <span data-ttu-id="44dcc-160">Service Fabric detecteert automatisch dergelijke fouten, met inbegrip van gevallen waarin Hallo machine geïsoleerd van andere machines vanwege toonetwork problemen wordt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-160">Service Fabric automatically detects these types of failures, including cases where hello machine becomes isolated from other machines due toonetwork issues.</span></span>

<span data-ttu-id="44dcc-161">Ongeacht Hallo-type van de service, één exemplaar resultaten in uitgevoerd uitvaltijd voor de service als dat één exemplaar van Hallo code om welke reden mislukt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-161">Regardless of hello type of service, running a single instance results in downtime for that service if that single copy of hello code fails for any reason.</span></span> 

<span data-ttu-id="44dcc-162">In de volgorde toohandle één enkele storing Hallo eenvoudigste wat die u kunt doen is tooensure die uw services worden uitgevoerd op meer dan één knooppunt standaard.</span><span class="sxs-lookup"><span data-stu-id="44dcc-162">In order toohandle any single failure, hello simplest thing you can do is tooensure that your services run on more than one node by default.</span></span> <span data-ttu-id="44dcc-163">Voor stateless services, kan dit worden bereikt door een `InstanceCount` groter is dan 1.</span><span class="sxs-lookup"><span data-stu-id="44dcc-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="44dcc-164">Voor stateful services Hallo minimale aanbeveling is altijd een `TargetReplicaSetSize` en `MinReplicaSetSize` van ten minste 3.</span><span class="sxs-lookup"><span data-stu-id="44dcc-164">For stateful services, hello minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="44dcc-165">Meer kopieën van uw servicecode wordt uitgevoerd, zorgt u ervoor dat uw service automatisch één enkele storing kan verwerken.</span><span class="sxs-lookup"><span data-stu-id="44dcc-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="44dcc-166">Verwerking gecoördineerd fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-166">Handling coordinated failures</span></span>
<span data-ttu-id="44dcc-167">Gecoördineerde fouten kunnen gebeuren in een cluster vervaldatum tooeither gepland of niet-geplande infrastructuur fouten en wijzigingen of wijzigingen in de geplande software.</span><span class="sxs-lookup"><span data-stu-id="44dcc-167">Coordinated failures can happen in a cluster due tooeither planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="44dcc-168">Service Fabric modellen infrastructuur-zones die gecoördineerde fouten als domeinen met fouten optreden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="44dcc-169">Gebieden die gecoördineerde softwarewijzigingen merken zijn gemodelleerd als domeinen upgraden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="44dcc-170">Meer informatie over probleem- en upgrade domeinen zich in [dit document](service-fabric-cluster-resource-manager-cluster-description.md) die beschrijft clustertopologie en definitie.</span><span class="sxs-lookup"><span data-stu-id="44dcc-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="44dcc-171">Standaard beschouwt Service Fabric-fout- en upgrade-domeinen bij het plannen waar uw services moeten worden uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="44dcc-172">Standaard probeert de Service Fabric tooensure die uw services worden uitgevoerd in verschillende domeinen voor probleem- en dus als gepland of ongepland wijzigingen per ongeluk uw services beschikbaar blijven.</span><span class="sxs-lookup"><span data-stu-id="44dcc-172">By default, Service Fabric tries tooensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="44dcc-173">Stel dat dat falen van een stroombron zorgt ervoor een rek met machines toofail tegelijk dat.</span><span class="sxs-lookup"><span data-stu-id="44dcc-173">For example, let's say that failure of a power source causes a rack of machines toofail simultaneously.</span></span> <span data-ttu-id="44dcc-174">Hiermee schakelt u in een ander voorbeeld van een storing voor een bepaalde service met meerdere exemplaren van Hallo-service Hallo verlies van veel computers in een domein-fout veroorzaakt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-174">With multiple copies of hello service running hello loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="44dcc-175">Dit is daarom essentieel tooensuring hoge beschikbaarheid van uw services beheren domeinen met fouten is.</span><span class="sxs-lookup"><span data-stu-id="44dcc-175">This is why managing fault domains is critical tooensuring high availability of your services.</span></span> <span data-ttu-id="44dcc-176">Wanneer het Service Fabric in Azure wordt uitgevoerd, worden foutdomeinen automatisch beheerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="44dcc-177">In andere omgevingen ze mogelijk niet.</span><span class="sxs-lookup"><span data-stu-id="44dcc-177">In other environments they may not be.</span></span> <span data-ttu-id="44dcc-178">Als u uw eigen clusters on-premises maakt, ervoor toomap en uw domein veroorzaakt indeling onjuist plant.</span><span class="sxs-lookup"><span data-stu-id="44dcc-178">If you're building your own clusters on premises, be sure toomap and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="44dcc-179">Upgradedomeinen zijn handig voor het modelleren gebieden waar software gaat toobe bijgewerkt op Hallo dezelfde tijd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-179">Upgrade Domains are useful for modeling areas where software is going toobe upgraded at hello same time.</span></span> <span data-ttu-id="44dcc-180">Als gevolg hiervan definiëren domeinen upgraden ook vaak Hallo grenzen waar software wordt offline gezet tijdens geplande upgrades.</span><span class="sxs-lookup"><span data-stu-id="44dcc-180">Because of this, Upgrade Domains also often define hello boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="44dcc-181">Upgrades van zowel Service Fabric en uw services Volg Hallo hetzelfde model.</span><span class="sxs-lookup"><span data-stu-id="44dcc-181">Upgrades of both Service Fabric and your services follow hello same model.</span></span> <span data-ttu-id="44dcc-182">Zie voor meer informatie over upgrades, upgradedomeinen en Hallo Service Fabric-statusmodel die voorkomt dat onbedoelde wijzigingen die invloed hebben op Hallo-cluster en uw service rolling deze documenten:</span><span class="sxs-lookup"><span data-stu-id="44dcc-182">For more on rolling upgrades, upgrade domains, and hello Service Fabric health model that helps prevent unintended changes from impacting hello cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="44dcc-183">Upgrade van de toepassing</span><span class="sxs-lookup"><span data-stu-id="44dcc-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="44dcc-184">Zelfstudie over Upgrade</span><span class="sxs-lookup"><span data-stu-id="44dcc-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="44dcc-185">Service Fabric-statusmodel</span><span class="sxs-lookup"><span data-stu-id="44dcc-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="44dcc-186">U kunt visualiseren Hallo-indeling van het cluster met behulp van Hallo clustertoewijzing opgegeven in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="44dcc-186">You can visualize hello layout of your cluster using hello cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="44dcc-187"><center>
![Knooppunten die zijn verdeeld over de domeinen met fouten in Service Fabric Explorer][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="44dcc-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="44dcc-188">Modeling gebieden mislukt, rolling upgrades met veel exemplaren van uw servicecode en status, plaatsing regels tooensure uw services uitvoeren in domeinen met fouten en upgrade, en de ingebouwde statuscontrole NET **sommige** Hallo functies die Service Fabric in de volgorde tookeep normale operationele problemen en fouten uit te schakelen in noodsituaties bevat.</span><span class="sxs-lookup"><span data-stu-id="44dcc-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules tooensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of hello features that Service Fabric provides in order tookeep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="44dcc-189">Verwerking van gelijktijdige hardware of software-fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="44dcc-190">Hierboven besproken we enkele fouten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-190">Above we talked about single failures.</span></span> <span data-ttu-id="44dcc-191">Zoals u ziet, zijn eenvoudig toohandle voor zowel stateless als stateful services alleen doordat meer exemplaren van het Hallo-code (en de status) uitvoert via domeinen met fouten en upgradedomeinen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-191">As you can see, are easy toohandle for both stateless and stateful services just by keeping more copies of hello code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="44dcc-192">Meerdere gelijktijdige willekeurige fouten kunnen ook gebeuren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="44dcc-193">Dit zijn vaker toolead tooan de werkelijke na noodgevallen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-193">These are more likely toolead tooan actual disaster.</span></span>


### <a name="random-failures-leading-tooservice-failures"></a><span data-ttu-id="44dcc-194">Willekeurige fouten voorloopspaties tooservice fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-194">Random failures leading tooservice failures</span></span>
<span data-ttu-id="44dcc-195">Stel dat Hallo-service heeft een `InstanceCount` van 5 en verschillende knooppunten die exemplaren met alle mislukte op Hallo dezelfde tijd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-195">Let's say that hello service had an `InstanceCount` of 5, and several nodes running those instances all failed at hello same time.</span></span> <span data-ttu-id="44dcc-196">Service Fabric reageert automatisch vervangen door exemplaren te maken op andere knooppunten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="44dcc-197">Vervangingen maken totdat het Hallo-service is weer terug tooits gewenst aantal exemplaren wordt voortgezet.</span><span class="sxs-lookup"><span data-stu-id="44dcc-197">It will continue creating replacements until hello service is back tooits desired instance count.</span></span> <span data-ttu-id="44dcc-198">Een ander voorbeeld: Stel dat er is een staatloze service met een `InstanceCount`van -1, wat betekent dat deze wordt uitgevoerd op alle geldige knooppunten in het Hallo-cluster.</span><span class="sxs-lookup"><span data-stu-id="44dcc-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in hello cluster.</span></span> <span data-ttu-id="44dcc-199">Stel dat een aantal van deze exemplaren toofail waren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-199">Let's say that some of those instances were toofail.</span></span> <span data-ttu-id="44dcc-200">In dit geval merkt Service Fabric Hallo-service is niet in de gewenste status en probeert toocreate Hallo-exemplaren op Hallo knooppunten waar ze ontbreekt zijn.</span><span class="sxs-lookup"><span data-stu-id="44dcc-200">In this case, Service Fabric notices that hello service is not in its desired state, and tries toocreate hello instances on hello nodes where they are missing.</span></span> 

<span data-ttu-id="44dcc-201">Voor stateful services Hallo situatie is afhankelijk van of Hallo-service heeft gehandhaafd status of niet.</span><span class="sxs-lookup"><span data-stu-id="44dcc-201">For stateful services hello situation depends on whether hello service has persisted state or not.</span></span> <span data-ttu-id="44dcc-202">Deze ook afhankelijk van hoeveel replica's Hallo-service heeft en hoeveel is mislukt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-202">It also depends on how many replicas hello service had and how many failed.</span></span> <span data-ttu-id="44dcc-203">Bepalen of een noodgeval is opgetreden voor een stateful service bevat en het beheren van deze drie fasen:</span><span class="sxs-lookup"><span data-stu-id="44dcc-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="44dcc-204">Bepalen of er quorumverlies of niet is</span><span class="sxs-lookup"><span data-stu-id="44dcc-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="44dcc-205">Een quorumverlies is elk gewenst moment een meerderheid van Hallo replica's van een stateful service lopen Hallo hetzelfde moment, met inbegrip van Hallo primaire.</span><span class="sxs-lookup"><span data-stu-id="44dcc-205">A quorum loss is any time a majority of hello replicas of a stateful service are down at hello same time, including hello Primary.</span></span>
2. <span data-ttu-id="44dcc-206">Bepalen of Hallo quorumverlies permanente of niet is</span><span class="sxs-lookup"><span data-stu-id="44dcc-206">Determining if hello quorum loss is permanent or not</span></span>
 - <span data-ttu-id="44dcc-207">De meeste tijd hello, zijn fouten tijdelijke.</span><span class="sxs-lookup"><span data-stu-id="44dcc-207">Most of hello time, failures are transient.</span></span> <span data-ttu-id="44dcc-208">Processen opnieuw zijn opgestart, opnieuw worden opgestart, virtuele machines zijn uitgebracht, retoucheren netwerkpartities.</span><span class="sxs-lookup"><span data-stu-id="44dcc-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="44dcc-209">Fouten zijn soms echter permanent.</span><span class="sxs-lookup"><span data-stu-id="44dcc-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="44dcc-210">Voor services zonder permanente status, een storing van een quorum of meer van de resultaten van de replica's _onmiddellijk_ permanente quorum verloren gaan.</span><span class="sxs-lookup"><span data-stu-id="44dcc-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="44dcc-211">Wanneer het Service Fabric quorumverlies in een stateful service voor niet-permanente detecteert, loopt hij onmiddellijk dataloss toostep 3 door te declareren (potentiële).</span><span class="sxs-lookup"><span data-stu-id="44dcc-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds toostep 3 by declaring (potential) dataloss.</span></span> <span data-ttu-id="44dcc-212">Procedure toodataloss zin omdat Service Fabric weet dat er geen punt in afwachting Hallo replica's toocome vorige, omdat zelfs als ze zijn hersteld, deze leeg zijn zou.</span><span class="sxs-lookup"><span data-stu-id="44dcc-212">Proceeding toodataloss makes sense because Service Fabric knows that there's no point in waiting for hello replicas toocome back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="44dcc-213">Een storing van een quorum of meer van de replica's voor permanente stateful services zorgt ervoor dat Service Fabric-toostart wacht op Hallo replica's toocome terug en terugzetten quorum.</span><span class="sxs-lookup"><span data-stu-id="44dcc-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric toostart waiting for hello replicas toocome back and restore quorum.</span></span> <span data-ttu-id="44dcc-214">Dit resulteert in een onderbreking van deze service voor een _schrijft_ toohello van invloed op een partitie (of 'replicaset') van Hallo-service.</span><span class="sxs-lookup"><span data-stu-id="44dcc-214">This results in a service outage for any _writes_ toohello affected partition (or "replica set") of hello service.</span></span> <span data-ttu-id="44dcc-215">Leesbewerkingen kunnen echter nog wel worden mogelijk met verminderde consistentie wordt gegarandeerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="44dcc-216">Hallo standaardhoeveelheid tijd dat het Service Fabric quorum toobe hersteld wacht is oneindig, omdat u doorgaat een (mogelijke) dataloss gebeurtenis en andere risico's uitvoert.</span><span class="sxs-lookup"><span data-stu-id="44dcc-216">hello default amount of time that Service Fabric waits for quorum toobe restored is infinite, since proceeding is a (potential) dataloss event and carries other risks.</span></span> <span data-ttu-id="44dcc-217">Hallo standaard overschrijven `QuorumLossWaitDuration` waarde is mogelijk, maar wordt niet aanbevolen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-217">Overriding hello default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="44dcc-218">In plaats daarvan op dit moment moeten alle pogingen worden aangebracht toorestore Hallo omlaag replica's.</span><span class="sxs-lookup"><span data-stu-id="44dcc-218">Instead at this time, all efforts should be made toorestore hello down replicas.</span></span> <span data-ttu-id="44dcc-219">Hiervoor moet het Hallo-knooppunten die omlaag back up te brengen, en ervoor te zorgen dat ze opnieuw kunnen koppelen Hallo stations waar ze Hallo lokale permanente status opgeslagen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-219">This requires bringing hello nodes that are down back up, and ensuring that they can remount hello drives where they stored hello local persistent state.</span></span> <span data-ttu-id="44dcc-220">Hallo quorumverlies wordt veroorzaakt door een fout, Service Fabric automatisch probeert toorecreate Hallo processen als Hallo replica's in deze opnieuw.</span><span class="sxs-lookup"><span data-stu-id="44dcc-220">If hello quorum loss is caused by process failure, Service Fabric automatically tries toorecreate hello processes and restart hello replicas inside them.</span></span> <span data-ttu-id="44dcc-221">Als dit mislukt, wordt in Service Fabric health fouten rapporteert.</span><span class="sxs-lookup"><span data-stu-id="44dcc-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="44dcc-222">Als deze omgezet worden kunnen Ga Hallo replica's gewoonlijk dan terug.</span><span class="sxs-lookup"><span data-stu-id="44dcc-222">If these can be resolved then hello replicas usually come back.</span></span> <span data-ttu-id="44dcc-223">Soms echter niet kunnen Hallo replica's worden teruggebracht.</span><span class="sxs-lookup"><span data-stu-id="44dcc-223">Sometimes, though, hello replicas can't be brought back.</span></span> <span data-ttu-id="44dcc-224">Bijvoorbeeld, Hallo stations mogelijk alle is mislukt of Hallo machines fysiek vernietigd enigszins.</span><span class="sxs-lookup"><span data-stu-id="44dcc-224">For example, hello drives may all have failed, or hello machines physically destroyed somehow.</span></span> <span data-ttu-id="44dcc-225">In dergelijke gevallen hebben we nu een permanente quorum verlies-gebeurtenis.</span><span class="sxs-lookup"><span data-stu-id="44dcc-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="44dcc-226">tootell Service Fabric toostop wachten op Hallo omlaag replica's toocome terug, Clusterbeheerder moet bepalen welke partities die services worden beïnvloed en roep Hallo `Repair-ServiceFabricPartition -PartitionId` of ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span><span class="sxs-lookup"><span data-stu-id="44dcc-226">tootell Service Fabric toostop waiting for hello down replicas toocome back, a cluster administrator must determine which partitions of which services are affected and call hello `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="44dcc-227">Deze API kunt u opgeven Hallo partitie toomove buiten QuorumLoss en potentiële dataloss Hallo-ID.</span><span class="sxs-lookup"><span data-stu-id="44dcc-227">This API allows specifying hello ID of hello partition toomove out of QuorumLoss and into potential dataloss.</span></span>

> [!NOTE]
> <span data-ttu-id="44dcc-228">Het is _nooit_ veilige toouse deze API anders dan in een bepaalde manier tegen specifieke partities.</span><span class="sxs-lookup"><span data-stu-id="44dcc-228">It is _never_ safe toouse this API other than in a targeted way against specific partitions.</span></span> 
>

3. <span data-ttu-id="44dcc-229">Bepalen of er sprake is van feitelijke gegevens verloren gaan en het terugzetten van back-ups</span><span class="sxs-lookup"><span data-stu-id="44dcc-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="44dcc-230">Wanneer het Service Fabric Hallo aangeroepen `OnDataLossAsync` methode altijd omdat is _vermoed_ dataloss.</span><span class="sxs-lookup"><span data-stu-id="44dcc-230">When Service Fabric calls hello `OnDataLossAsync` method it is always because of _suspected_ dataloss.</span></span> <span data-ttu-id="44dcc-231">Service Fabric zorgt ervoor dat deze aanroep toohello wordt geleverd _aanbevolen_ resterende replica.</span><span class="sxs-lookup"><span data-stu-id="44dcc-231">Service Fabric ensures that this call is delivered toohello _best_ remaining replica.</span></span> <span data-ttu-id="44dcc-232">Dit is de replica voortgang is Hallo meeste.</span><span class="sxs-lookup"><span data-stu-id="44dcc-232">This is whichever replica has made hello most progress.</span></span> <span data-ttu-id="44dcc-233">Hallo reden we altijd spreken _vermoed_ dataloss is dat het is mogelijk alle dezelfde staat die Hallo resterende replica daadwerkelijk heeft als Hallo primaire was voordat deze werd afgesloten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-233">hello reason we always say _suspected_ dataloss is that it is possible that hello remaining replica actually has all same state as hello Primary did when it went down.</span></span> <span data-ttu-id="44dcc-234">Echter, zonder dat toocompare staat het, is er geen goede manier voor Service Fabric of operators tooknow controleren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-234">However, without that state toocompare it to, there's no good way for Service Fabric or operators tooknow for sure.</span></span> <span data-ttu-id="44dcc-235">Op dit moment Service Fabric weet ook Hallo andere replica's zijn niet terugkomen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-235">At this point, Service Fabric also knows hello other replicas are not coming back.</span></span> <span data-ttu-id="44dcc-236">Dat was Hallo beslissingen bij het wachten op Hallo quorum verlies tooresolve zelf is gestopt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-236">That was hello decision made when we stopped waiting for hello quorum loss tooresolve itself.</span></span> <span data-ttu-id="44dcc-237">Hallo oplossing komen voor Hallo-service is meestal toofreeze en wacht tot specifieke interventie van een beheerder.</span><span class="sxs-lookup"><span data-stu-id="44dcc-237">hello best course of action for hello service is usually toofreeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="44dcc-238">Dus de betekenis van een typische implementatie van Hallo `OnDataLossAsync` methode doen?</span><span class="sxs-lookup"><span data-stu-id="44dcc-238">So what does a typical implementation of hello `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="44dcc-239">Meld u eerst die `OnDataLossAsync` is geactiveerd en eventuele benodigde beheerdersrechten waarschuwingen starten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="44dcc-240">Meestal op dit moment toopause en wacht voor verdere beslissingen en handmatige acties toobe genomen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-240">Usually at this point, toopause and wait for further decisions and manual actions toobe taken.</span></span> <span data-ttu-id="44dcc-241">Dit is omdat ook als back-ups beschikbaar toobe voorbereid moeten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-241">This is because even if backups are available they may need toobe prepared.</span></span> <span data-ttu-id="44dcc-242">Bijvoorbeeld, als twee verschillende services coördineren van gegevens, mogelijk deze back-ups toobe gewijzigd in de volgorde tooensure die zodra Hallo terugzetten Hallo informatie die twee services belang gebeurt bij consistent is.</span><span class="sxs-lookup"><span data-stu-id="44dcc-242">For example, if two different services coordinate information, those backups may need toobe modified in order tooensure that once hello restore happens that hello information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="44dcc-243">Vaak is ook andere telemetrie of uitlaatgas van Hallo-service.</span><span class="sxs-lookup"><span data-stu-id="44dcc-243">Often there is also some other telemetry or exhaust from hello service.</span></span> <span data-ttu-id="44dcc-244">Deze metagegevens kan zijn opgenomen in de andere services of in Logboeken.</span><span class="sxs-lookup"><span data-stu-id="44dcc-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="44dcc-245">Deze informatie kan gebruikte benodigde toodetermine zijn als er aanroepen ontvangen en verwerkt op primaire Hallo niet aanwezig zijn in bepaalde Hallo back-up of gerepliceerde toothis-replica zijn.</span><span class="sxs-lookup"><span data-stu-id="44dcc-245">This information can be used needed toodetermine if there were any calls received and processed at hello primary that were not present in hello backup or replicated toothis particular replica.</span></span> <span data-ttu-id="44dcc-246">Deze wellicht toobe herhaald of toegevoegde toohello back-up voordat de herstelbewerking haalbaar is.</span><span class="sxs-lookup"><span data-stu-id="44dcc-246">These may need toobe replayed or added toohello backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="44dcc-247">Vergelijkingen van Hallo resterende van replica de status toothat opgenomen in alle back-ups die beschikbaar zijn.</span><span class="sxs-lookup"><span data-stu-id="44dcc-247">Comparisons of hello remaining replica's state toothat contained in any backups that are available.</span></span> <span data-ttu-id="44dcc-248">Als met betrouwbare Service Fabric-verzamelingen hello wordt er zijn extra en beschikbaar verwerkt om te doen, dat wordt beschreven in [in dit artikel](service-fabric-reliable-services-backup-restore.md).</span><span class="sxs-lookup"><span data-stu-id="44dcc-248">If using hello Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="44dcc-249">Hallo-doel is toosee als Hallo status binnen Hallo replica voldoende is of ook welke Hallo back-up ontbreekt mogelijk.</span><span class="sxs-lookup"><span data-stu-id="44dcc-249">hello goal is toosee if hello state within hello replica is sufficient, or also what hello backup may be missing.</span></span>
  - <span data-ttu-id="44dcc-250">Eenmaal Hallo vergelijking wordt uitgevoerd, en als de benodigde Hallo terugzetten is voltooid, Hallo servicecode moet true wordt geretourneerd als er wijzigingen zijn aangebracht.</span><span class="sxs-lookup"><span data-stu-id="44dcc-250">Once hello comparison is done, and if necessary hello restore completed, hello service code should return true if any state changes were made.</span></span> <span data-ttu-id="44dcc-251">Retourneert onwaar als de replica Hallo vastgesteld dat het was Hallo beste beschikbaar exemplaar van Hallo status heeft en er zijn geen wijzigingen aangebracht.</span><span class="sxs-lookup"><span data-stu-id="44dcc-251">If hello replica determined that it was hello best available copy of hello state and made no changes, then return false.</span></span> <span data-ttu-id="44dcc-252">Waar geeft aan dat elke _andere_ resterende replica's nu mogelijk niet door deze gegevensset.</span><span class="sxs-lookup"><span data-stu-id="44dcc-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="44dcc-253">Ze zullen worden verwijderd en opnieuw opgebouwd uit deze replica.</span><span class="sxs-lookup"><span data-stu-id="44dcc-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="44dcc-254">Drempelwaarde.ONWAAR geeft aan dat er geen statuswijzigingen doorgevoerd, dus Hallo andere replica's kunnen houden wat ze hebben.</span><span class="sxs-lookup"><span data-stu-id="44dcc-254">False indicates that no state changes were made, so hello other replicas can keep what they have.</span></span> 

<span data-ttu-id="44dcc-255">Het is zeer belangrijk dat service auteurs potentiële dataloss en scenario's fouten oefenen voordat services ooit worden geïmplementeerd in de productieomgeving.</span><span class="sxs-lookup"><span data-stu-id="44dcc-255">It is critically important that service authors practice potential dataloss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="44dcc-256">tooprotect tegen dataloss Hallo mogelijkheid is het belangrijk tooperiodically [back-up Hallo status](service-fabric-reliable-services-backup-restore.md) van elk van uw stateful services tooa geografisch redundante opslag.</span><span class="sxs-lookup"><span data-stu-id="44dcc-256">tooprotect against hello possibility of dataloss, it is important tooperiodically [back up hello state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services tooa geo-redundant store.</span></span> <span data-ttu-id="44dcc-257">U moet ook voor zorgen dat er Hallo mogelijkheid toorestore deze.</span><span class="sxs-lookup"><span data-stu-id="44dcc-257">You must also ensure that you have hello ability toorestore it.</span></span> <span data-ttu-id="44dcc-258">Aangezien de back-ups van veel verschillende services worden uitgevoerd op verschillende tijdstippen, moet u tooensure dat de uw services na een herstelbewerking een consistente weergave van elkaar hebben.</span><span class="sxs-lookup"><span data-stu-id="44dcc-258">Since backups of many different services are taken at different times, you need tooensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="44dcc-259">Neem bijvoorbeeld een situatie waarin een service een aantal genereert en opgeslagen en vervolgens verzendt het tooanother-service waarmee gegevens worden ook opgeslagen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-259">For example, consider a situation where one service generates a number and stores it, then sends it tooanother service that also stores it.</span></span> <span data-ttu-id="44dcc-260">Na een herstelbewerking mogelijk ontdekt u dat tweede Hallo-service Hallo nummer heeft maar Hallo eerst niet bestaat, omdat de back-up die voor deze bewerking niet bevatten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-260">After a restore, you might discover that hello second service has hello number but hello first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="44dcc-261">Als u dat Hallo resterende ontdekken replica's zijn onvoldoende toocontinue uit in een scenario dataloss en u kan geen servicestatus van telemetrie of uitlaatgas Reconstrueer, de best mogelijke beoogd herstelpunt (RPO) wordt bepaald door Hallo frequentie van uw back-ups .</span><span class="sxs-lookup"><span data-stu-id="44dcc-261">If you find out that hello remaining replicas are insufficient toocontinue from in a dataloss scenario, and you can't reconstruct service state from telemetry or exhaust, hello frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="44dcc-262">Service Fabric bevat veel hulpprogramma's voor het testen van verschillende scenario's voor mislukt, met inbegrip van permanente quorum en dataloss herstel vanaf een back-up vereisen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and dataloss requiring restoration from a backup.</span></span> <span data-ttu-id="44dcc-263">Deze scenario's zijn opgenomen als onderdeel van de Service Fabric testbaarheid-hulpprogramma's worden beheerd door Hallo veroorzaakt Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="44dcc-263">These scenarios are included as a part of Service Fabric's testability tools, managed by hello Fault Analysis Service.</span></span> <span data-ttu-id="44dcc-264">Meer informatie over deze hulpprogramma's en patronen [hier](service-fabric-testability-overview.md).</span><span class="sxs-lookup"><span data-stu-id="44dcc-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="44dcc-265">Systeemservices kunnen ook quorum verloren zijn gegaan, Hallo gevolgen wordt specifieke toohello service betrokken afnemen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-265">System services can also suffer quorum loss, with hello impact being specific toohello service in question.</span></span> <span data-ttu-id="44dcc-266">Bijvoorbeeld quorumverlies in Hallo naamgevingsservice heeft gevolgen voor naamomzetting, terwijl quorumverlies in Hallo failover manager service maken van nieuwe services en failovers geblokkeerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-266">For instance, quorum loss in hello naming service impacts name resolution, whereas quorum loss in hello failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="44dcc-267">Tijdens het Hallo Service Fabric-systeemservices volgen hetzelfde patroon als uw services voor statusbeheer hello, dit wordt niet aanbevolen dat u toomove moet proberen ze buiten Quorumverlies en potentiële dataloss.</span><span class="sxs-lookup"><span data-stu-id="44dcc-267">While hello Service Fabric system services follow hello same pattern as your services for state management, it is not recommended that you should attempt toomove them out of Quorum Loss and into potential dataloss.</span></span> <span data-ttu-id="44dcc-268">Hallo-aanbeveling is in plaats daarvan te[ondersteuning zoeken](service-fabric-support.md) toodetermine een oplossing die is gericht tooyour specifieke situatie.</span><span class="sxs-lookup"><span data-stu-id="44dcc-268">hello recommendation is instead too[seek support](service-fabric-support.md) toodetermine a solution that is targeted tooyour specific situation.</span></span>  <span data-ttu-id="44dcc-269">Meestal is het beter toosimply wacht tot Hallo omlaag replica's retourneren.</span><span class="sxs-lookup"><span data-stu-id="44dcc-269">Usually it is preferable toosimply wait until hello down replicas return.</span></span>
>

## <a name="availability-of-hello-service-fabric-cluster"></a><span data-ttu-id="44dcc-270">Beschikbaarheid van Hallo Service Fabric-cluster</span><span class="sxs-lookup"><span data-stu-id="44dcc-270">Availability of hello Service Fabric cluster</span></span>
<span data-ttu-id="44dcc-271">Hallo Service Fabric-cluster zichzelf is over het algemeen een sterk gedistribueerde omgeving met geen enkele storingspunten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-271">Generally speaking, hello Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="44dcc-272">Een storing van een willekeurig knooppunt leidt niet tot de beschikbaarheid of betrouwbaarheidsproblemen voor Hallo-cluster, voornamelijk omdat Hallo Service Fabric-systeemservices Volg Hallo dezelfde richtlijnen die eerder is verkregen: ze altijd uitgevoerd met drie of meer replica's standaard en die systeemservices die staatloze zijn uitgevoerd op alle knooppunten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-272">A failure of any one node will not cause availability or reliability issues for hello cluster, primarily because hello Service Fabric system services follow hello same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="44dcc-273">Hallo onderliggende Service Fabric-netwerken en fout detectie lagen worden volledig gedistribueerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-273">hello underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="44dcc-274">De meeste systeemservices van metagegevens in Hallo cluster kan worden gemaakt of weten hoe tooresynchronize hun status vanuit andere locaties.</span><span class="sxs-lookup"><span data-stu-id="44dcc-274">Most system services can be rebuilt from metadata in hello cluster, or know how tooresynchronize their state from other places.</span></span> <span data-ttu-id="44dcc-275">Hallo-beschikbaarheid van Hallo cluster kan niet meer betrouwbaar als systeemservices quorum verlies situaties zoals deze beschreven bovenstaande krijgen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-275">hello availability of hello cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="44dcc-276">In dergelijke gevallen niet mogelijk kunnen tooperform bepaalde bewerkingen op Hallo-cluster dat begint met een upgrade of implementeren van nieuwe services, maar het Hallo-cluster zelf nog steeds actief is.</span><span class="sxs-lookup"><span data-stu-id="44dcc-276">In these cases you may not be able tooperform certain operations on hello cluster like starting an upgrade or deploying new services, but hello cluster itself is still up.</span></span> <span data-ttu-id="44dcc-277">Services op het al wordt uitgevoerd blijft actief in deze voorwaarden, tenzij ze nodig schrijfbewerkingen toohello system services toocontinue werkt hebben.</span><span class="sxs-lookup"><span data-stu-id="44dcc-277">Services on already running will remain running in these conditions unless they require writes toohello system services toocontinue functioning.</span></span> <span data-ttu-id="44dcc-278">Bijvoorbeeld als Hallo Failover Manager is sprake van quorumverlies alle services toorun wordt voortgezet, maar kunnen services die niet voldoen aan zich niet kunnen tooautomatically opnieuw wordt opgestart, omdat dit Hallo betrokkenheid van Hallo Failover Manager vereist.</span><span class="sxs-lookup"><span data-stu-id="44dcc-278">For example, if hello Failover Manager is in quorum loss all services will continue toorun, but any services that fail will not be able tooautomatically restart, since this requires hello involvement of hello Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="44dcc-279">Fouten van een datacenter of een Azure-regio</span><span class="sxs-lookup"><span data-stu-id="44dcc-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="44dcc-280">In zeldzame gevallen, een fysieke datacenter zijn tijdelijk niet beschikbaar vanwege tooloss van power of netwerkverbinding.</span><span class="sxs-lookup"><span data-stu-id="44dcc-280">In rare cases, a physical data center can become temporarily unavailable due tooloss of power or network connectivity.</span></span> <span data-ttu-id="44dcc-281">In deze gevallen is is de Service Fabric-clusters en -services in het desbetreffende datacenter of een Azure-regio niet beschikbaar.</span><span class="sxs-lookup"><span data-stu-id="44dcc-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="44dcc-282">Echter, _uw gegevens behouden blijven_.</span><span class="sxs-lookup"><span data-stu-id="44dcc-282">However, _your data is preserved_.</span></span> <span data-ttu-id="44dcc-283">Voor clusters worden uitgevoerd in Azure, kunt u updates bekijken op storingen op Hallo [pagina Azure status][azure-status-dashboard].</span><span class="sxs-lookup"><span data-stu-id="44dcc-283">For clusters running in Azure, you can view updates on outages on hello [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="44dcc-284">Zeer onwaarschijnlijk gebeurtenis die een fysieke datacenter is volledig of gedeeltelijk vernietigd, alle Service Fabric-clusters die er worden gehost in Hallo of Hallo-services in deze verloren kunnen gaan.</span><span class="sxs-lookup"><span data-stu-id="44dcc-284">In hello highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or hello services inside them could be lost.</span></span> <span data-ttu-id="44dcc-285">Dit omvat geen status die niet een back-up buiten het datacenter of de regio.</span><span class="sxs-lookup"><span data-stu-id="44dcc-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="44dcc-286">Er is twee verschillende strategieën voor functionerende hallo permanente of volgehouden falen van een enkele datacenter of regio.</span><span class="sxs-lookup"><span data-stu-id="44dcc-286">There's two different strategies for surviving hello permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="44dcc-287">Afzonderlijke Service Fabric-clusters worden uitgevoerd in meerdere dergelijke regio's en gebruikmaken van een mechanisme voor failover en failback tussen deze omgevingen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="44dcc-288">De sortering van meerdere actief-actief of actief / passief clustermodel vereist aanvullende code voor beheer van en bewerkingen.</span><span class="sxs-lookup"><span data-stu-id="44dcc-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="44dcc-289">Dit is ook vereist coördinatie van de back-ups van Hallo-services in een datacenter of regio zodat ze beschikbaar zijn in andere datacenters of de regio's wanneer een mislukt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-289">This also requires coordination of backups from hello services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="44dcc-290">Voer één Service Fabric-cluster die meerdere datacenters of regio's omvat.</span><span class="sxs-lookup"><span data-stu-id="44dcc-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="44dcc-291">Hallo minimale ondersteunde configuratie voor deze is drie datacenters of regio's.</span><span class="sxs-lookup"><span data-stu-id="44dcc-291">hello minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="44dcc-292">Hallo aanbevolen aantal regio's of datacenters is vijf.</span><span class="sxs-lookup"><span data-stu-id="44dcc-292">hello recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="44dcc-293">Hiervoor moet een complexere clustertopologie.</span><span class="sxs-lookup"><span data-stu-id="44dcc-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="44dcc-294">Hallo is voordeel van dit model echter dat falen van een datacenter of regio na een noodgeval is geconverteerd naar een normale fout.</span><span class="sxs-lookup"><span data-stu-id="44dcc-294">However, hello benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="44dcc-295">Deze fouten kunnen worden verwerkt door het Hallo-mechanismen die werken voor clusters binnen één regio.</span><span class="sxs-lookup"><span data-stu-id="44dcc-295">These failures can be handled by hello mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="44dcc-296">Zorg ervoor werkbelastingen worden gedistribueerd, zodat ze normaal fouten tolereren domeinen met fouten en upgradedomeinen regels voor de plaatsing van de Service Fabric.</span><span class="sxs-lookup"><span data-stu-id="44dcc-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="44dcc-297">Voor meer informatie over beleidsregels die kunnen helpen bij het gebruiken van services in dit type cluster Lees informatie over [plaatsingsbeleid](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="44dcc-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-toocluster-failures"></a><span data-ttu-id="44dcc-298">Willekeurige fouten voorloopspaties toocluster fouten</span><span class="sxs-lookup"><span data-stu-id="44dcc-298">Random failures leading toocluster failures</span></span>
<span data-ttu-id="44dcc-299">Service Fabric heeft Hallo concept van Seed-knooppunten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-299">Service Fabric has hello concept of Seed Nodes.</span></span> <span data-ttu-id="44dcc-300">Dit zijn de knooppunten die de beschikbaarheid van de onderliggende cluster Hallo Hallo onderhouden.</span><span class="sxs-lookup"><span data-stu-id="44dcc-300">These are nodes that maintain hello availability of hello underlying cluster.</span></span> <span data-ttu-id="44dcc-301">Deze knooppunten te up maken van tooensure Hallo cluster blijft door tot stand brengen van leases met andere knooppunten en fungeren als tiebreakers tijdens bepaalde soorten netwerkfouten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-301">These nodes help tooensure hello cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="44dcc-302">Als willekeurige fouten een meerderheid van Hallo seed-knooppunten in cluster Hallo verwijderen en ze niet worden toegevoegd, Hallo cluster automatisch afgesloten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-302">If random failures remove a majority of hello seed nodes in hello cluster and they are not brought back, hello cluster automatically shuts down.</span></span> <span data-ttu-id="44dcc-303">In Azure, Seed-knooppunten automatisch worden beheerd: ze worden gedistribueerd via Hallo beschikbaar fouten en upgradedomeinen en als een enkel seed-knooppunt wordt verwijderd uit Hallo cluster een andere naam in plaats daarvan wordt gemaakt.</span><span class="sxs-lookup"><span data-stu-id="44dcc-303">In Azure, Seed Nodes are automatically managed: they are distributed over hello available fault and upgrade domains, and if a single seed node is removed from hello cluster another one will be created in its place.</span></span> 

<span data-ttu-id="44dcc-304">In zelfstandige Service Fabric-clusters en Azure is Hallo 'Primaire knooppunttype' hello die Hallo zaden wordt uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-304">In both standalone Service Fabric clusters and Azure, hello "Primary Node Type" is hello one that runs hello seeds.</span></span> <span data-ttu-id="44dcc-305">Bij het definiëren van een primaire knooppunttype Service Fabric, automatisch profiteren van het aantal knooppunten dat is geleverd door het maken van too9 seed-knooppunten en 9 replica's van elk van de systeemservices Hallo Hallo.</span><span class="sxs-lookup"><span data-stu-id="44dcc-305">When defining a primary node type, Service Fabric will automatically take advantage of hello number of nodes provided by creating up too9 seed nodes and 9 replicas of each of hello system services.</span></span> <span data-ttu-id="44dcc-306">Als een set van willekeurige fouten tegelijkertijd uit een meerderheid van die replica van de service system duurt, Voer Hallo systeemservices quorum verloren zijn gegaan, zoals hierboven is beschreven.</span><span class="sxs-lookup"><span data-stu-id="44dcc-306">If a set of random failures takes out a majority of those system service replicas simultaneously, hello system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="44dcc-307">Als een meerderheid van Hallo seed-knooppunten verloren gaan, Hallo cluster snel na afgesloten.</span><span class="sxs-lookup"><span data-stu-id="44dcc-307">If a majority of hello seed nodes are lost, hello cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="44dcc-308">Volgende stappen</span><span class="sxs-lookup"><span data-stu-id="44dcc-308">Next steps</span></span>
- <span data-ttu-id="44dcc-309">Meer informatie over hoe toosimulate diverse fouten met Hallo [testbaarheid framework](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="44dcc-309">Learn how toosimulate various failures using hello [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="44dcc-310">Lezen van andere bronnen voor herstel na noodgevallen en hoge beschikbaarheid.</span><span class="sxs-lookup"><span data-stu-id="44dcc-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="44dcc-311">Microsoft heeft een grote hoeveelheid informatie over deze onderwerpen worden gepubliceerd.</span><span class="sxs-lookup"><span data-stu-id="44dcc-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="44dcc-312">Terwijl sommige van deze documenten toospecific technieken voor gebruik in andere producten verwijzen, bevatten ze veel algemene aanbevolen procedures die u in Hallo Service Fabric-context ook toepassen kunt:</span><span class="sxs-lookup"><span data-stu-id="44dcc-312">While some of these documents refer toospecific techniques for use in other products, they contain many general best practices you can apply in hello Service Fabric context as well:</span></span>
  - [<span data-ttu-id="44dcc-313">Beschikbaarheidscontrolelijst</span><span class="sxs-lookup"><span data-stu-id="44dcc-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="44dcc-314">Uitvoeren van een herstel na noodgevallen detailanalyse</span><span class="sxs-lookup"><span data-stu-id="44dcc-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="44dcc-315">[Herstel na noodgevallen en hoge beschikbaarheid voor Azure-toepassingen][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="44dcc-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="44dcc-316">Meer informatie over [ondersteuningsopties voor Service Fabric](service-fabric-support.md)</span><span class="sxs-lookup"><span data-stu-id="44dcc-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
